{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark COde Practice"
      ],
      "metadata": {
        "id": "c2L-0y-LDFIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "dVz3VjwEDKo_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Saprk Session\n",
        "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "_tiWFtKPDTmN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"1/1/2023\", \"C1\", 20),\n",
        "    (\"1/1/2023\", \"C2\", 20),\n",
        "    (\"1/2/2023\", \"C2\", 50),\n",
        "    (\"1/2/2023\", \"C3\", 12),\n",
        "    (\"1/3/2023\", \"C4\", 20),\n",
        "    (\"1/3/2023\", \"C5\", 100),\n",
        "    (\"1/3/2023\", \"C1\", 123),\n",
        "]\n",
        "column = ['Date_fld', 'Custome_name', 'Amount']\n",
        "\n",
        "df = spark.createDataFrame(data, column)\n",
        "df.show()\n",
        "\n",
        "# Convert date in proper format\n",
        "df = df.withColumn(\"Date_fld\", to_date(col(\"Date_fld\"), \"M/d/yyyy\"))\n",
        "df.show()\n",
        "\n",
        "# Define window partition by customer orderd by date\n",
        "window_space = Window.partitionBy('Custome_name').orderBy('Date_fld')\n",
        "\n",
        "#Assign row number within each customer number\n",
        "df = df.withColumn('Row_Number', row_number().over(window_space))\n",
        "df.show()\n",
        "\n",
        "#filter only new customer\n",
        "df_new_custome = df.filter(col('Row_Number') == 1)\n",
        "df_new_custome.show()\n",
        "\n",
        "# Count distinct new customers per date\n",
        "result_df = df_new_custome.groupBy('Date_fld').agg(countDistinct('Custome_name').alias('Price_Count'))\n",
        "result_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQ59NvmKDZ3i",
        "outputId": "e4a10e89-9308-43bb-8c15-5984a058ce01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+------+\n",
            "|Date_fld|Custome_name|Amount|\n",
            "+--------+------------+------+\n",
            "|1/1/2023|          C1|    20|\n",
            "|1/1/2023|          C2|    20|\n",
            "|1/2/2023|          C2|    50|\n",
            "|1/2/2023|          C3|    12|\n",
            "|1/3/2023|          C4|    20|\n",
            "|1/3/2023|          C5|   100|\n",
            "|1/3/2023|          C1|   123|\n",
            "+--------+------------+------+\n",
            "\n",
            "+----------+------------+------+\n",
            "|  Date_fld|Custome_name|Amount|\n",
            "+----------+------------+------+\n",
            "|2023-01-01|          C1|    20|\n",
            "|2023-01-01|          C2|    20|\n",
            "|2023-01-02|          C2|    50|\n",
            "|2023-01-02|          C3|    12|\n",
            "|2023-01-03|          C4|    20|\n",
            "|2023-01-03|          C5|   100|\n",
            "|2023-01-03|          C1|   123|\n",
            "+----------+------------+------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-03|          C1|   123|         2|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C2|    50|         2|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|  Date_fld|Price_Count|\n",
            "+----------+-----------+\n",
            "|2023-01-01|          2|\n",
            "|2023-01-02|          1|\n",
            "|2023-01-03|          2|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark code to solve"
      ],
      "metadata": {
        "id": "xFATEYxtcqFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Craete Spark Session\n",
        "spark2 = SparkSession\\\n",
        ".builder\\\n",
        ".config('spark.shuffle.useOldFetchers', 'true')\\\n",
        ".config('spark.ui.port','0')\\\n",
        ".config('spark.sql.warehouse.dir', '/user/itv008042/warehouse')\\\n",
        ".enableHiveSupport()\\\n",
        ".master('yarn')\\\n",
        ".appName('PySparkPractice')\\\n",
        ".getOrCreate()\n",
        "\n",
        "#Create Schema\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"ActorId\", IntegerType(), True),\n",
        "    StructField(\"DirectorId\", IntegerType(), True),\n",
        "    StructField(\"TimeStamp\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (1,1,0),\n",
        "    (1,1,1),\n",
        "    (1,1,2),\n",
        "    (1,2,3),\n",
        "    (1,2,4),\n",
        "    (1,1,5),\n",
        "    (1,1,6)\n",
        "]\n",
        "\n",
        "# Create Data frame\n",
        "df = spark2.createDataFrame(data, schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "TfJPZ3n2ECGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f003ada8-a37a-4b19-cc68-c61d248741fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+\n",
            "|ActorId|DirectorId|TimeStamp|\n",
            "+-------+----------+---------+\n",
            "|      1|         1|        0|\n",
            "|      1|         1|        1|\n",
            "|      1|         1|        2|\n",
            "|      1|         2|        3|\n",
            "|      1|         2|        4|\n",
            "|      1|         1|        5|\n",
            "|      1|         1|        6|\n",
            "+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Group By ActorId','DirectorId"
      ],
      "metadata": {
        "id": "YiiczhnSfO8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_group = df.groupBy('ActorId', 'DirectorId').count()\n",
        "df_group.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjzrc2scw4h",
        "outputId": "d1e06f5b-2ce7-4cac-eb56-0a6c6e98ccc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "|      1|         2|    2|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_group.filter(df_group['count']> 3).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvPYNvKfnvN",
        "outputId": "ba1da6f9-695d-4b0a-a9a6-56dd24555c20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02: Write an pyspark code to find the ctr of each Ad.Round ctr to 2\n",
        "decimal points. Order the result table by ctr in descending order\n",
        "and by ad_id in ascending order in case of a tie."
      ],
      "metadata": {
        "id": "JgBHsJkFicj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"ad_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"action\", StringType(), True)\n",
        "])\n",
        "data = [\n",
        "   (1, 1, 'Clicked'),\n",
        "  (2, 2, 'Clicked'),\n",
        "  (3, 3, 'Viewed'),\n",
        "  (5, 5, 'Ignored'),\n",
        "  (1, 7, 'Ignored'),\n",
        "  (2, 7, 'Viewed'),\n",
        "  (3, 5, 'Clicked'),\n",
        "  (1, 4, 'Viewed'),\n",
        "  (2, 11, 'Viewed'),\n",
        "  (1, 2, 'Clicked')\n",
        "]\n",
        "\n",
        "#Create Datframe\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFAj-6DliTqs",
        "outputId": "411cdbda-c76d-436d-81eb-d7747974a91c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+\n",
            "|ad_id|user_id| action|\n",
            "+-----+-------+-------+\n",
            "|    1|      1|Clicked|\n",
            "|    2|      2|Clicked|\n",
            "|    3|      3| Viewed|\n",
            "|    5|      5|Ignored|\n",
            "|    1|      7|Ignored|\n",
            "|    2|      7| Viewed|\n",
            "|    3|      5|Clicked|\n",
            "|    1|      4| Viewed|\n",
            "|    2|     11| Viewed|\n",
            "|    1|      2|Clicked|\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03: Write a Pyspark program to report the first name, last name, city, and state of each person in the\n",
        "Person dataframe. If the address of a personId is not present in the Address dataframe,\n",
        "report null instead."
      ],
      "metadata": {
        "id": "DmaD35cTq3NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schema for the 'persons' table\n",
        "persons_schema = StructType([\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"lastName\", StringType(), True),\n",
        "  StructField(\"firstName\", StringType(), True)\n",
        "])\n",
        "# Define schema for the 'addresses' table\n",
        "addresses_schema = StructType([\n",
        "  StructField(\"addressId\", IntegerType(), True),\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"city\", StringType(), True),\n",
        "  StructField(\"state\", StringType(), True)\n",
        "])\n",
        "# Define data for the 'persons' table\n",
        "persons_data = [\n",
        "  (1, 'Wang', 'Allen'),\n",
        "  (2, 'Alice', 'Bob')\n",
        "]\n",
        "# Define data for the 'addresses' table\n",
        "addresses_data = [\n",
        "  (1, 2, 'New York City', 'New York'),\n",
        "  (2, 3, 'Leetcode', 'California')\n",
        "]\n",
        "\n",
        "#Create Data Frame\n",
        "person_df = spark.createDataFrame(persons_data, persons_schema)\n",
        "address_df = spark.createDataFrame(addresses_data, addresses_schema)\n",
        "\n",
        "person_df.show()\n",
        "address_df.show()\n"
      ],
      "metadata": {
        "id": "pYsdT38Vqt2o",
        "outputId": "4ee0df32-c360-44c0-c7ce-09bf9d82ef16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+---------+\n",
            "|personId|lastName|firstName|\n",
            "+--------+--------+---------+\n",
            "|       1|    Wang|    Allen|\n",
            "|       2|   Alice|      Bob|\n",
            "+--------+--------+---------+\n",
            "\n",
            "+---------+--------+-------------+----------+\n",
            "|addressId|personId|         city|     state|\n",
            "+---------+--------+-------------+----------+\n",
            "|        1|       2|New York City|  New York|\n",
            "|        2|       3|     Leetcode|California|\n",
            "+---------+--------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = person_df.join(address_df, person_df[\"personId\"] == address_df[\"personId\"], \"left\")\n",
        "df_join.select('firstName', 'lastName','city','state').show()\n",
        "\n",
        "##first name, last name, city, and state"
      ],
      "metadata": {
        "id": "vutIvI1lrWwT",
        "outputId": "07560923-5139-4e61-fd3e-d12d5ab874e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------------+--------+\n",
            "|firstName|lastName|         city|   state|\n",
            "+---------+--------+-------------+--------+\n",
            "|    Allen|    Wang|         NULL|    NULL|\n",
            "|      Bob|   Alice|New York City|New York|\n",
            "+---------+--------+-------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "04: Employees Earning More Than Their Managers\n",
        "Write a Pyspark program to find Employees Earning More Than Their\n",
        "Managers"
      ],
      "metadata": {
        "id": "DtZVuMHessY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"employees\"\n",
        "employees_schema = StructType([\n",
        "  StructField(\"id\", IntegerType(), True),\n",
        "  StructField(\"name\", StringType(), True),\n",
        "  StructField(\"salary\", IntegerType(), True),\n",
        "  StructField(\"managerId\", IntegerType(), True)\n",
        "])\n",
        "# Define data for the \"employees\"\n",
        "employees_data = [\n",
        "  (1, 'Joe', 70000, 3),\n",
        "  (2, 'Henry', 80000, 4),\n",
        "  (3, 'Sam', 60000, None),\n",
        "  (4, 'Max', 90000, None)\n",
        "]\n",
        "\n",
        "emp_df = spark.createDataFrame(employees_data, employees_schema)\n",
        "emp_df.show()\n"
      ],
      "metadata": {
        "id": "SSTRUyDCr4KP",
        "outputId": "763b34a2-8ff0-4e54-be93-c213c57c6c01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+---------+\n",
            "| id| name|salary|managerId|\n",
            "+---+-----+------+---------+\n",
            "|  1|  Joe| 70000|        3|\n",
            "|  2|Henry| 80000|        4|\n",
            "|  3|  Sam| 60000|     NULL|\n",
            "|  4|  Max| 90000|     NULL|\n",
            "+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df_1 = emp_df.alias('e1')\n",
        "emp_df_2 = emp_df.alias('e2')\n",
        "\n",
        "self_joined_df = emp_df_1.join(emp_df_2, col(\"e1.id\") == col(\"e2.managerId\"))\n",
        "self_joined_df.show()\n"
      ],
      "metadata": {
        "id": "ZUS5MtKctLWM",
        "outputId": "a59f5109-2fd0-45bd-dd1b-a075eb6516ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+---------+---+-----+------+---------+\n",
            "| id|name|salary|managerId| id| name|salary|managerId|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "|  3| Sam| 60000|     NULL|  1|  Joe| 70000|        3|\n",
            "|  4| Max| 90000|     NULL|  2|Henry| 80000|        4|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df = self_joined_df.select(col(\"e2.name\"), col('e2.salary'), col(\"e1.salary\").alias(\"manager_salary\"))\n",
        "self_joined_after_df.show()"
      ],
      "metadata": {
        "id": "pAwgyERSuYrh",
        "outputId": "bec89265-0373-4f27-83b8-95fdbd471dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------------+\n",
            "| name|salary|manager_salary|\n",
            "+-----+------+--------------+\n",
            "|  Joe| 70000|         60000|\n",
            "|Henry| 80000|         90000|\n",
            "+-----+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df.filter(self_joined_after_df.salary > self_joined_after_df.manager_salary).select('name').show()"
      ],
      "metadata": {
        "id": "ciBHiMBRvAB4",
        "outputId": "b174fae1-fe71-4fb5-e7fc-89677406afd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| Joe|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Pyspark program to report all the duplicate emails.\n",
        "Note that it's guaranteed that the email field is not NULL."
      ],
      "metadata": {
        "id": "lxncLIOe0nlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"emails\" table\n",
        "emails_schema = StructType([\n",
        "StructField(\"id\", IntegerType(), True),\n",
        "StructField(\"email\", StringType(), True)\n",
        "])\n",
        "# Define data for the \"emails\" table\n",
        "emails_data = [\n",
        "(1, 'a@b.com'),\n",
        "(2, 'c@d.com'),\n",
        "(3, 'a@b.com')\n",
        "]\n",
        "\n",
        "# Create a PySpark DataFrame\n",
        "email_df = spark.createDataFrame(emails_data, emails_schema)\n",
        "email_df.show()\n"
      ],
      "metadata": {
        "id": "WupwVbyFvTA2",
        "outputId": "ed3a8c83-ea10-4e0c-8d50-129ba8dcdc69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|  email|\n",
            "+---+-------+\n",
            "|  1|a@b.com|\n",
            "|  2|c@d.com|\n",
            "|  3|a@b.com|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_email_group = email_df.groupBy('email').count()\n",
        "df_email_group.show()\n",
        "\n",
        "df_email_group.filter(df_email_group['count']>1).show()"
      ],
      "metadata": {
        "id": "yhSQ-MF-1On_",
        "outputId": "525f2bfa-d193-4d3a-df99-1d0d32446d23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "|c@d.com|    1|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to start page 21"
      ],
      "metadata": {
        "id": "WCakKP5s2AOy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JtWqBxS1Yp-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a program in pyspark last 3 days product sell rolling avarage price?"
      ],
      "metadata": {
        "id": "ga_2A6oe9hbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: product_id, date, price\n",
        "data = [\n",
        "    (\"p1\", \"2025-04-10\", 100),\n",
        "    (\"p1\", \"2025-04-11\", 110),\n",
        "    (\"p1\", \"2025-04-12\", 105),\n",
        "    (\"p1\", \"2025-04-13\", 120),\n",
        "    (\"p1\", \"2025-04-14\", 130),\n",
        "    (\"p2\", \"2025-04-10\", 200),\n",
        "    (\"p2\", \"2025-04-11\", 220),\n",
        "    (\"p2\", \"2025-04-12\", 210),\n",
        "    (\"p2\", \"2025-04-14\", 230),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"date\", \"price\"])\n",
        "# Convert date to DateType\n",
        "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Define window: partition by product_id, ordered by date, looking back 2 rows (3-day window)\n",
        "windowSpace = Window.partitionBy(\"product_id\").orderBy(\"date\").rowsBetween(-2, 0)\n",
        "\n",
        "# Add rolling average column\n",
        "df_with_rolling_avg_column = df.withColumn(\"rolling_avg_price\", avg(\"price\").over(windowSpace))\n",
        "\n",
        "df_with_rolling_avg_column.orderBy(\"product_id\", \"date\").show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUBpXEBm93vg",
        "outputId": "ca4afeb4-29db-4bea-dd8f-3038b5751cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+\n",
            "|product_id|      date|price|\n",
            "+----------+----------+-----+\n",
            "|        p1|2025-04-10|  100|\n",
            "|        p1|2025-04-11|  110|\n",
            "|        p1|2025-04-12|  105|\n",
            "|        p1|2025-04-13|  120|\n",
            "|        p1|2025-04-14|  130|\n",
            "|        p2|2025-04-10|  200|\n",
            "|        p2|2025-04-11|  220|\n",
            "|        p2|2025-04-12|  210|\n",
            "|        p2|2025-04-14|  230|\n",
            "+----------+----------+-----+\n",
            "\n",
            "+----------+----------+-----+------------------+\n",
            "|product_id|      date|price| rolling_avg_price|\n",
            "+----------+----------+-----+------------------+\n",
            "|        p1|2025-04-10|  100|             100.0|\n",
            "|        p1|2025-04-11|  110|             105.0|\n",
            "|        p1|2025-04-12|  105|             105.0|\n",
            "|        p1|2025-04-13|  120|111.66666666666667|\n",
            "|        p1|2025-04-14|  130|118.33333333333333|\n",
            "|        p2|2025-04-10|  200|             200.0|\n",
            "|        p2|2025-04-11|  220|             210.0|\n",
            "|        p2|2025-04-12|  210|             210.0|\n",
            "|        p2|2025-04-14|  230|             220.0|\n",
            "+----------+----------+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re given a dataset containing employee names, departments, and their salaries.\n",
        "\n",
        " Your task is to rank the employees by salary within each department and classify them as:\n",
        "\n",
        "\"High Growth\" if the salary is higher than the previous person in that department\n",
        "\n",
        "\"Low Growth\" if the salary is lower\n",
        "\n",
        "\"No Growth\" if the salary is the same or it's the first employee in the department"
      ],
      "metadata": {
        "id": "EgPFWB8HyfBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Alice\", \"HR\", 50000),\n",
        "    (\"Bob\", \"IT\", 60000),\n",
        "    (\"Charlie\", \"HR\", 55000),\n",
        "    (\"David\", \"IT\", 70000),\n",
        "    (\"Eve\", \"HR\", 52000),\n",
        "    (\"Frank\", \"IT\", 65000),\n",
        "    (\"Grace\", \"HR\", 51000),\n",
        "    (\"Hank\", \"IT\", 68000),\n",
        "    (\"Ivy\", \"HR\", 53000),\n",
        "    (\"Jack\", \"IT\", 72000),\n",
        "    (\"Kelly\", \"HR\", 54000),\n",
        "    (\"Luke\", \"IT\", 69000),\n",
        "\n",
        "]\n",
        "columns = ['name', 'department', 'salary']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Defin window function and use LAG to compare the salary\n",
        "windowSpace = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "df_with_lag = df.withColumn(\"previous_salary\", lag(\"salary\").over(windowSpace))\n",
        "df_with_lag.show()\n",
        "\n",
        "#Apply Classification Logic\n",
        "final_df = df_with_lag.withColumn(\n",
        "    \"growth_status\",\n",
        "    when(col(\"previous_salary\").isNull(), \"No Growth\").when(col(\"salary\")>col(\"previous_salary\"), \"High Growth\").when(col(\"salary\")<col(\"previous_salary\"), \"Low Growth\").otherwise(\"No Growth\")\n",
        ")\n",
        "\n",
        "#Display the result\n",
        "final_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NOCxkBOM-AlO",
        "outputId": "ac9a1929-7904-4eeb-f423-c936aad1042e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|   name|department|salary|\n",
            "+-------+----------+------+\n",
            "|  Alice|        HR| 50000|\n",
            "|    Bob|        IT| 60000|\n",
            "|Charlie|        HR| 55000|\n",
            "|  David|        IT| 70000|\n",
            "|    Eve|        HR| 52000|\n",
            "|  Frank|        IT| 65000|\n",
            "|  Grace|        HR| 51000|\n",
            "|   Hank|        IT| 68000|\n",
            "|    Ivy|        HR| 53000|\n",
            "|   Jack|        IT| 72000|\n",
            "|  Kelly|        HR| 54000|\n",
            "|   Luke|        IT| 69000|\n",
            "+-------+----------+------+\n",
            "\n",
            "+-------+----------+------+---------------+\n",
            "|   name|department|salary|previous_salary|\n",
            "+-------+----------+------+---------------+\n",
            "|  Alice|        HR| 50000|           NULL|\n",
            "|  Grace|        HR| 51000|          50000|\n",
            "|    Eve|        HR| 52000|          51000|\n",
            "|    Ivy|        HR| 53000|          52000|\n",
            "|  Kelly|        HR| 54000|          53000|\n",
            "|Charlie|        HR| 55000|          54000|\n",
            "|    Bob|        IT| 60000|           NULL|\n",
            "|  Frank|        IT| 65000|          60000|\n",
            "|   Hank|        IT| 68000|          65000|\n",
            "|   Luke|        IT| 69000|          68000|\n",
            "|  David|        IT| 70000|          69000|\n",
            "|   Jack|        IT| 72000|          70000|\n",
            "+-------+----------+------+---------------+\n",
            "\n",
            "+-------+----------+------+---------------+-------------+\n",
            "|   name|department|salary|previous_salary|growth_status|\n",
            "+-------+----------+------+---------------+-------------+\n",
            "|  Alice|        HR| 50000|           NULL|    No Growth|\n",
            "|  Grace|        HR| 51000|          50000|  High Growth|\n",
            "|    Eve|        HR| 52000|          51000|  High Growth|\n",
            "|    Ivy|        HR| 53000|          52000|  High Growth|\n",
            "|  Kelly|        HR| 54000|          53000|  High Growth|\n",
            "|Charlie|        HR| 55000|          54000|  High Growth|\n",
            "|    Bob|        IT| 60000|           NULL|    No Growth|\n",
            "|  Frank|        IT| 65000|          60000|  High Growth|\n",
            "|   Hank|        IT| 68000|          65000|  High Growth|\n",
            "|   Luke|        IT| 69000|          68000|  High Growth|\n",
            "|  David|        IT| 70000|          69000|  High Growth|\n",
            "|   Jack|        IT| 72000|          70000|  High Growth|\n",
            "+-------+----------+------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAimnEH1zIG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}