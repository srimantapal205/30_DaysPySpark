{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark COde Practice"
      ],
      "metadata": {
        "id": "c2L-0y-LDFIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "dVz3VjwEDKo_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Saprk Session\n",
        "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "_tiWFtKPDTmN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"1/1/2023\", \"C1\", 20),\n",
        "    (\"1/1/2023\", \"C2\", 20),\n",
        "    (\"1/2/2023\", \"C2\", 50),\n",
        "    (\"1/2/2023\", \"C3\", 12),\n",
        "    (\"1/3/2023\", \"C4\", 20),\n",
        "    (\"1/3/2023\", \"C5\", 100),\n",
        "    (\"1/3/2023\", \"C1\", 123),\n",
        "]\n",
        "column = ['Date_fld', 'Custome_name', 'Amount']\n",
        "\n",
        "df = spark.createDataFrame(data, column)\n",
        "df.show()\n",
        "\n",
        "# Convert date in proper format\n",
        "df = df.withColumn(\"Date_fld\", to_date(col(\"Date_fld\"), \"M/d/yyyy\"))\n",
        "df.show()\n",
        "\n",
        "# Define window partition by customer orderd by date\n",
        "window_space = Window.partitionBy('Custome_name').orderBy('Date_fld')\n",
        "\n",
        "#Assign row number within each customer number\n",
        "df = df.withColumn('Row_Number', row_number().over(window_space))\n",
        "df.show()\n",
        "\n",
        "#filter only new customer\n",
        "df_new_custome = df.filter(col('Row_Number') == 1)\n",
        "df_new_custome.show()\n",
        "\n",
        "# Count distinct new customers per date\n",
        "result_df = df_new_custome.groupBy('Date_fld').agg(countDistinct('Custome_name').alias('Price_Count'))\n",
        "result_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQ59NvmKDZ3i",
        "outputId": "89f6946b-720c-43cf-9a23-7a3e023288c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+------+\n",
            "|Date_fld|Custome_name|Amount|\n",
            "+--------+------------+------+\n",
            "|1/1/2023|          C1|    20|\n",
            "|1/1/2023|          C2|    20|\n",
            "|1/2/2023|          C2|    50|\n",
            "|1/2/2023|          C3|    12|\n",
            "|1/3/2023|          C4|    20|\n",
            "|1/3/2023|          C5|   100|\n",
            "|1/3/2023|          C1|   123|\n",
            "+--------+------------+------+\n",
            "\n",
            "+----------+------------+------+\n",
            "|  Date_fld|Custome_name|Amount|\n",
            "+----------+------------+------+\n",
            "|2023-01-01|          C1|    20|\n",
            "|2023-01-01|          C2|    20|\n",
            "|2023-01-02|          C2|    50|\n",
            "|2023-01-02|          C3|    12|\n",
            "|2023-01-03|          C4|    20|\n",
            "|2023-01-03|          C5|   100|\n",
            "|2023-01-03|          C1|   123|\n",
            "+----------+------------+------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-03|          C1|   123|         2|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C2|    50|         2|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|  Date_fld|Price_Count|\n",
            "+----------+-----------+\n",
            "|2023-01-01|          2|\n",
            "|2023-01-02|          1|\n",
            "|2023-01-03|          2|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark code to solve"
      ],
      "metadata": {
        "id": "xFATEYxtcqFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Craete Spark Session\n",
        "spark2 = SparkSession\\\n",
        ".builder\\\n",
        ".config('spark.shuffle.useOldFetchers', 'true')\\\n",
        ".config('spark.ui.port','0')\\\n",
        ".config('spark.sql.warehouse.dir', '/user/itv008042/warehouse')\\\n",
        ".enableHiveSupport()\\\n",
        ".master('yarn')\\\n",
        ".appName('PySparkPractice')\\\n",
        ".getOrCreate()\n",
        "\n",
        "#Create Schema\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"ActorId\", IntegerType(), True),\n",
        "    StructField(\"DirectorId\", IntegerType(), True),\n",
        "    StructField(\"TimeStamp\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (1,1,0),\n",
        "    (1,1,1),\n",
        "    (1,1,2),\n",
        "    (1,2,3),\n",
        "    (1,2,4),\n",
        "    (1,1,5),\n",
        "    (1,1,6)\n",
        "]\n",
        "\n",
        "# Create Data frame\n",
        "df = spark2.createDataFrame(data, schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "TfJPZ3n2ECGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7c81a1-0e4a-4869-c5a9-3255bffc3a42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+\n",
            "|ActorId|DirectorId|TimeStamp|\n",
            "+-------+----------+---------+\n",
            "|      1|         1|        0|\n",
            "|      1|         1|        1|\n",
            "|      1|         1|        2|\n",
            "|      1|         2|        3|\n",
            "|      1|         2|        4|\n",
            "|      1|         1|        5|\n",
            "|      1|         1|        6|\n",
            "+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Group By ActorId','DirectorId"
      ],
      "metadata": {
        "id": "YiiczhnSfO8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_group = df.groupBy('ActorId', 'DirectorId').count()\n",
        "df_group.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjzrc2scw4h",
        "outputId": "5668de1d-0a3b-4616-e4c7-99fb1f3f03dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "|      1|         2|    2|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_group.filter(df_group['count']> 3).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvPYNvKfnvN",
        "outputId": "e66ba79a-0fcd-4fb7-d108-6e695440c44d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02: Write an pyspark code to find the ctr of each Ad.Round ctr to 2\n",
        "decimal points. Order the result table by ctr in descending order\n",
        "and by ad_id in ascending order in case of a tie."
      ],
      "metadata": {
        "id": "JgBHsJkFicj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"ad_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"action\", StringType(), True)\n",
        "])\n",
        "data = [\n",
        "   (1, 1, 'Clicked'),\n",
        "  (2, 2, 'Clicked'),\n",
        "  (3, 3, 'Viewed'),\n",
        "  (5, 5, 'Ignored'),\n",
        "  (1, 7, 'Ignored'),\n",
        "  (2, 7, 'Viewed'),\n",
        "  (3, 5, 'Clicked'),\n",
        "  (1, 4, 'Viewed'),\n",
        "  (2, 11, 'Viewed'),\n",
        "  (1, 2, 'Clicked')\n",
        "]\n",
        "\n",
        "#Create Datframe\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFAj-6DliTqs",
        "outputId": "01ce112e-d1fe-4463-89ca-621e7fdc15fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+\n",
            "|ad_id|user_id| action|\n",
            "+-----+-------+-------+\n",
            "|    1|      1|Clicked|\n",
            "|    2|      2|Clicked|\n",
            "|    3|      3| Viewed|\n",
            "|    5|      5|Ignored|\n",
            "|    1|      7|Ignored|\n",
            "|    2|      7| Viewed|\n",
            "|    3|      5|Clicked|\n",
            "|    1|      4| Viewed|\n",
            "|    2|     11| Viewed|\n",
            "|    1|      2|Clicked|\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03: Write a Pyspark program to report the first name, last name, city, and state of each person in the\n",
        "Person dataframe. If the address of a personId is not present in the Address dataframe,\n",
        "report null instead."
      ],
      "metadata": {
        "id": "DmaD35cTq3NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schema for the 'persons' table\n",
        "persons_schema = StructType([\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"lastName\", StringType(), True),\n",
        "  StructField(\"firstName\", StringType(), True)\n",
        "])\n",
        "# Define schema for the 'addresses' table\n",
        "addresses_schema = StructType([\n",
        "  StructField(\"addressId\", IntegerType(), True),\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"city\", StringType(), True),\n",
        "  StructField(\"state\", StringType(), True)\n",
        "])\n",
        "# Define data for the 'persons' table\n",
        "persons_data = [\n",
        "  (1, 'Wang', 'Allen'),\n",
        "  (2, 'Alice', 'Bob')\n",
        "]\n",
        "# Define data for the 'addresses' table\n",
        "addresses_data = [\n",
        "  (1, 2, 'New York City', 'New York'),\n",
        "  (2, 3, 'Leetcode', 'California')\n",
        "]\n",
        "\n",
        "#Create Data Frame\n",
        "person_df = spark.createDataFrame(persons_data, persons_schema)\n",
        "address_df = spark.createDataFrame(addresses_data, addresses_schema)\n",
        "\n",
        "person_df.show()\n",
        "address_df.show()\n"
      ],
      "metadata": {
        "id": "pYsdT38Vqt2o",
        "outputId": "72050766-beb6-4e8e-c336-33c2b2c15b9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+---------+\n",
            "|personId|lastName|firstName|\n",
            "+--------+--------+---------+\n",
            "|       1|    Wang|    Allen|\n",
            "|       2|   Alice|      Bob|\n",
            "+--------+--------+---------+\n",
            "\n",
            "+---------+--------+-------------+----------+\n",
            "|addressId|personId|         city|     state|\n",
            "+---------+--------+-------------+----------+\n",
            "|        1|       2|New York City|  New York|\n",
            "|        2|       3|     Leetcode|California|\n",
            "+---------+--------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = person_df.join(address_df, person_df[\"personId\"] == address_df[\"personId\"], \"left\")\n",
        "df_join.select('firstName', 'lastName','city','state').show()\n",
        "\n",
        "##first name, last name, city, and state"
      ],
      "metadata": {
        "id": "vutIvI1lrWwT",
        "outputId": "fce4cf82-b2ad-4c49-9fe0-d3c2da9d7c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------------+--------+\n",
            "|firstName|lastName|         city|   state|\n",
            "+---------+--------+-------------+--------+\n",
            "|    Allen|    Wang|         NULL|    NULL|\n",
            "|      Bob|   Alice|New York City|New York|\n",
            "+---------+--------+-------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "04: Employees Earning More Than Their Managers\n",
        "Write a Pyspark program to find Employees Earning More Than Their\n",
        "Managers"
      ],
      "metadata": {
        "id": "DtZVuMHessY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"employees\"\n",
        "employees_schema = StructType([\n",
        "  StructField(\"id\", IntegerType(), True),\n",
        "  StructField(\"name\", StringType(), True),\n",
        "  StructField(\"salary\", IntegerType(), True),\n",
        "  StructField(\"managerId\", IntegerType(), True)\n",
        "])\n",
        "# Define data for the \"employees\"\n",
        "employees_data = [\n",
        "  (1, 'Joe', 70000, 3),\n",
        "  (2, 'Henry', 80000, 4),\n",
        "  (3, 'Sam', 60000, None),\n",
        "  (4, 'Max', 90000, None)\n",
        "]\n",
        "\n",
        "emp_df = spark.createDataFrame(employees_data, employees_schema)\n",
        "emp_df.show()\n"
      ],
      "metadata": {
        "id": "SSTRUyDCr4KP",
        "outputId": "c19c7cb3-c2e7-4649-a560-1b8acf7369a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+---------+\n",
            "| id| name|salary|managerId|\n",
            "+---+-----+------+---------+\n",
            "|  1|  Joe| 70000|        3|\n",
            "|  2|Henry| 80000|        4|\n",
            "|  3|  Sam| 60000|     NULL|\n",
            "|  4|  Max| 90000|     NULL|\n",
            "+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df_1 = emp_df.alias('e1')\n",
        "emp_df_2 = emp_df.alias('e2')\n",
        "\n",
        "self_joined_df = emp_df_1.join(emp_df_2, col(\"e1.id\") == col(\"e2.managerId\"))\n",
        "self_joined_df.show()\n"
      ],
      "metadata": {
        "id": "ZUS5MtKctLWM",
        "outputId": "63882a86-b8b9-4bfe-957c-b1fc76068238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+---------+---+-----+------+---------+\n",
            "| id|name|salary|managerId| id| name|salary|managerId|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "|  3| Sam| 60000|     NULL|  1|  Joe| 70000|        3|\n",
            "|  4| Max| 90000|     NULL|  2|Henry| 80000|        4|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df = self_joined_df.select(col(\"e2.name\"), col('e2.salary'), col(\"e1.salary\").alias(\"manager_salary\"))\n",
        "self_joined_after_df.show()"
      ],
      "metadata": {
        "id": "pAwgyERSuYrh",
        "outputId": "d1744c20-f7ee-4e22-b9b3-9905ef86a418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------------+\n",
            "| name|salary|manager_salary|\n",
            "+-----+------+--------------+\n",
            "|  Joe| 70000|         60000|\n",
            "|Henry| 80000|         90000|\n",
            "+-----+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df.filter(self_joined_after_df.salary > self_joined_after_df.manager_salary).select('name').show()"
      ],
      "metadata": {
        "id": "ciBHiMBRvAB4",
        "outputId": "c75fe016-5d04-4ec3-dfa0-fcc69035205e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| Joe|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Pyspark program to report all the duplicate emails.\n",
        "Note that it's guaranteed that the email field is not NULL."
      ],
      "metadata": {
        "id": "lxncLIOe0nlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"emails\" table\n",
        "emails_schema = StructType([\n",
        "StructField(\"id\", IntegerType(), True),\n",
        "StructField(\"email\", StringType(), True)\n",
        "])\n",
        "# Define data for the \"emails\" table\n",
        "emails_data = [\n",
        "(1, 'a@b.com'),\n",
        "(2, 'c@d.com'),\n",
        "(3, 'a@b.com')\n",
        "]\n",
        "\n",
        "# Create a PySpark DataFrame\n",
        "email_df = spark.createDataFrame(emails_data, emails_schema)\n",
        "email_df.show()\n"
      ],
      "metadata": {
        "id": "WupwVbyFvTA2",
        "outputId": "57159664-ec15-4a77-f480-a15be66c59fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|  email|\n",
            "+---+-------+\n",
            "|  1|a@b.com|\n",
            "|  2|c@d.com|\n",
            "|  3|a@b.com|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_email_group = email_df.groupBy('email').count()\n",
        "df_email_group.show()\n",
        "\n",
        "df_email_group.filter(df_email_group['count']>1).show()"
      ],
      "metadata": {
        "id": "yhSQ-MF-1On_",
        "outputId": "40b45601-79fd-4bd4-8bb1-fb8008a068eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "|c@d.com|    1|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to start page 21"
      ],
      "metadata": {
        "id": "WCakKP5s2AOy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JtWqBxS1Yp-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a program in pyspark last 3 days product sell rolling avarage price?"
      ],
      "metadata": {
        "id": "ga_2A6oe9hbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: product_id, date, price\n",
        "data = [\n",
        "    (\"p1\", \"2025-04-10\", 100),\n",
        "    (\"p1\", \"2025-04-11\", 110),\n",
        "    (\"p1\", \"2025-04-12\", 105),\n",
        "    (\"p1\", \"2025-04-13\", 120),\n",
        "    (\"p1\", \"2025-04-14\", 130),\n",
        "    (\"p2\", \"2025-04-10\", 200),\n",
        "    (\"p2\", \"2025-04-11\", 220),\n",
        "    (\"p2\", \"2025-04-12\", 210),\n",
        "    (\"p2\", \"2025-04-14\", 230),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"date\", \"price\"])\n",
        "# Convert date to DateType\n",
        "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Define window: partition by product_id, ordered by date, looking back 2 rows (3-day window)\n",
        "windowSpace = Window.partitionBy(\"product_id\").orderBy(\"date\").rowsBetween(-2, 0)\n",
        "\n",
        "# Add rolling average column\n",
        "df_with_rolling_avg_column = df.withColumn(\"rolling_avg_price\", avg(\"price\").over(windowSpace))\n",
        "\n",
        "df_with_rolling_avg_column.orderBy(\"product_id\", \"date\").show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUBpXEBm93vg",
        "outputId": "98a27a3c-8756-4960-c1ac-dd8459d93967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+\n",
            "|product_id|      date|price|\n",
            "+----------+----------+-----+\n",
            "|        p1|2025-04-10|  100|\n",
            "|        p1|2025-04-11|  110|\n",
            "|        p1|2025-04-12|  105|\n",
            "|        p1|2025-04-13|  120|\n",
            "|        p1|2025-04-14|  130|\n",
            "|        p2|2025-04-10|  200|\n",
            "|        p2|2025-04-11|  220|\n",
            "|        p2|2025-04-12|  210|\n",
            "|        p2|2025-04-14|  230|\n",
            "+----------+----------+-----+\n",
            "\n",
            "+----------+----------+-----+------------------+\n",
            "|product_id|      date|price| rolling_avg_price|\n",
            "+----------+----------+-----+------------------+\n",
            "|        p1|2025-04-10|  100|             100.0|\n",
            "|        p1|2025-04-11|  110|             105.0|\n",
            "|        p1|2025-04-12|  105|             105.0|\n",
            "|        p1|2025-04-13|  120|111.66666666666667|\n",
            "|        p1|2025-04-14|  130|118.33333333333333|\n",
            "|        p2|2025-04-10|  200|             200.0|\n",
            "|        p2|2025-04-11|  220|             210.0|\n",
            "|        p2|2025-04-12|  210|             210.0|\n",
            "|        p2|2025-04-14|  230|             220.0|\n",
            "+----------+----------+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOCxkBOM-AlO"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}