{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark COde Practice"
      ],
      "metadata": {
        "id": "c2L-0y-LDFIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "dVz3VjwEDKo_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Saprk Session\n",
        "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "_tiWFtKPDTmN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"1/1/2023\", \"C1\", 20),\n",
        "    (\"1/1/2023\", \"C2\", 20),\n",
        "    (\"1/2/2023\", \"C2\", 50),\n",
        "    (\"1/2/2023\", \"C3\", 12),\n",
        "    (\"1/3/2023\", \"C4\", 20),\n",
        "    (\"1/3/2023\", \"C5\", 100),\n",
        "    (\"1/3/2023\", \"C1\", 123),\n",
        "]\n",
        "column = ['Date_fld', 'Custome_name', 'Amount']\n",
        "\n",
        "df = spark.createDataFrame(data, column)\n",
        "df.show()\n",
        "\n",
        "# Convert date in proper format\n",
        "df = df.withColumn(\"Date_fld\", to_date(col(\"Date_fld\"), \"M/d/yyyy\"))\n",
        "df.show()\n",
        "\n",
        "# Define window partition by customer orderd by date\n",
        "window_space = Window.partitionBy('Custome_name').orderBy('Date_fld')\n",
        "\n",
        "#Assign row number within each customer number\n",
        "df = df.withColumn('Row_Number', row_number().over(window_space))\n",
        "df.show()\n",
        "\n",
        "#filter only new customer\n",
        "df_new_custome = df.filter(col('Row_Number') == 1)\n",
        "df_new_custome.show()\n",
        "\n",
        "# Count distinct new customers per date\n",
        "result_df = df_new_custome.groupBy('Date_fld').agg(countDistinct('Custome_name').alias('Price_Count'))\n",
        "result_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQ59NvmKDZ3i",
        "outputId": "4c422e18-8e98-4909-b9a8-b102d18777a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+------+\n",
            "|Date_fld|Custome_name|Amount|\n",
            "+--------+------------+------+\n",
            "|1/1/2023|          C1|    20|\n",
            "|1/1/2023|          C2|    20|\n",
            "|1/2/2023|          C2|    50|\n",
            "|1/2/2023|          C3|    12|\n",
            "|1/3/2023|          C4|    20|\n",
            "|1/3/2023|          C5|   100|\n",
            "|1/3/2023|          C1|   123|\n",
            "+--------+------------+------+\n",
            "\n",
            "+----------+------------+------+\n",
            "|  Date_fld|Custome_name|Amount|\n",
            "+----------+------------+------+\n",
            "|2023-01-01|          C1|    20|\n",
            "|2023-01-01|          C2|    20|\n",
            "|2023-01-02|          C2|    50|\n",
            "|2023-01-02|          C3|    12|\n",
            "|2023-01-03|          C4|    20|\n",
            "|2023-01-03|          C5|   100|\n",
            "|2023-01-03|          C1|   123|\n",
            "+----------+------------+------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-03|          C1|   123|         2|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C2|    50|         2|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|  Date_fld|Price_Count|\n",
            "+----------+-----------+\n",
            "|2023-01-01|          2|\n",
            "|2023-01-02|          1|\n",
            "|2023-01-03|          2|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark code to solve"
      ],
      "metadata": {
        "id": "xFATEYxtcqFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Craete Spark Session\n",
        "spark2 = SparkSession\\\n",
        ".builder\\\n",
        ".config('spark.shuffle.useOldFetchers', 'true')\\\n",
        ".config('spark.ui.port','0')\\\n",
        ".config('spark.sql.warehouse.dir', '/user/itv008042/warehouse')\\\n",
        ".enableHiveSupport()\\\n",
        ".master('yarn')\\\n",
        ".appName('PySparkPractice')\\\n",
        ".getOrCreate()\n",
        "\n",
        "#Create Schema\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"ActorId\", IntegerType(), True),\n",
        "    StructField(\"DirectorId\", IntegerType(), True),\n",
        "    StructField(\"TimeStamp\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (1,1,0),\n",
        "    (1,1,1),\n",
        "    (1,1,2),\n",
        "    (1,2,3),\n",
        "    (1,2,4),\n",
        "    (1,1,5),\n",
        "    (1,1,6)\n",
        "]\n",
        "\n",
        "# Create Data frame\n",
        "df = spark2.createDataFrame(data, schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "TfJPZ3n2ECGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4028c10a-7b08-4f67-c75a-1abaf78ed8e6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+\n",
            "|ActorId|DirectorId|TimeStamp|\n",
            "+-------+----------+---------+\n",
            "|      1|         1|        0|\n",
            "|      1|         1|        1|\n",
            "|      1|         1|        2|\n",
            "|      1|         2|        3|\n",
            "|      1|         2|        4|\n",
            "|      1|         1|        5|\n",
            "|      1|         1|        6|\n",
            "+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Group By ActorId','DirectorId"
      ],
      "metadata": {
        "id": "YiiczhnSfO8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_group = df.groupBy('ActorId', 'DirectorId').count()\n",
        "df_group.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjzrc2scw4h",
        "outputId": "59b38c3d-f62f-4b9d-de3a-d01bf2700581"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "|      1|         2|    2|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_group.filter(df_group['count']> 3).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvPYNvKfnvN",
        "outputId": "13b997b7-694d-4b44-8e68-2c5eea28a4f8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02: Write an pyspark code to find the ctr of each Ad.Round ctr to 2\n",
        "decimal points. Order the result table by ctr in descending order\n",
        "and by ad_id in ascending order in case of a tie."
      ],
      "metadata": {
        "id": "JgBHsJkFicj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"ad_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"action\", StringType(), True)\n",
        "])\n",
        "data = [\n",
        "   (1, 1, 'Clicked'),\n",
        "  (2, 2, 'Clicked'),\n",
        "  (3, 3, 'Viewed'),\n",
        "  (5, 5, 'Ignored'),\n",
        "  (1, 7, 'Ignored'),\n",
        "  (2, 7, 'Viewed'),\n",
        "  (3, 5, 'Clicked'),\n",
        "  (1, 4, 'Viewed'),\n",
        "  (2, 11, 'Viewed'),\n",
        "  (1, 2, 'Clicked')\n",
        "]\n",
        "\n",
        "#Create Datframe\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFAj-6DliTqs",
        "outputId": "69043a06-09ef-4925-c7bc-10c5e43af0d1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+\n",
            "|ad_id|user_id| action|\n",
            "+-----+-------+-------+\n",
            "|    1|      1|Clicked|\n",
            "|    2|      2|Clicked|\n",
            "|    3|      3| Viewed|\n",
            "|    5|      5|Ignored|\n",
            "|    1|      7|Ignored|\n",
            "|    2|      7| Viewed|\n",
            "|    3|      5|Clicked|\n",
            "|    1|      4| Viewed|\n",
            "|    2|     11| Viewed|\n",
            "|    1|      2|Clicked|\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03: Write a Pyspark program to report the first name, last name, city, and state of each person in the\n",
        "Person dataframe. If the address of a personId is not present in the Address dataframe,\n",
        "report null instead."
      ],
      "metadata": {
        "id": "DmaD35cTq3NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schema for the 'persons' table\n",
        "persons_schema = StructType([\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"lastName\", StringType(), True),\n",
        "  StructField(\"firstName\", StringType(), True)\n",
        "])\n",
        "# Define schema for the 'addresses' table\n",
        "addresses_schema = StructType([\n",
        "  StructField(\"addressId\", IntegerType(), True),\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"city\", StringType(), True),\n",
        "  StructField(\"state\", StringType(), True)\n",
        "])\n",
        "# Define data for the 'persons' table\n",
        "persons_data = [\n",
        "  (1, 'Wang', 'Allen'),\n",
        "  (2, 'Alice', 'Bob')\n",
        "]\n",
        "# Define data for the 'addresses' table\n",
        "addresses_data = [\n",
        "  (1, 2, 'New York City', 'New York'),\n",
        "  (2, 3, 'Leetcode', 'California')\n",
        "]\n",
        "\n",
        "#Create Data Frame\n",
        "person_df = spark.createDataFrame(persons_data, persons_schema)\n",
        "address_df = spark.createDataFrame(addresses_data, addresses_schema)\n",
        "\n",
        "person_df.show()\n",
        "address_df.show()\n"
      ],
      "metadata": {
        "id": "pYsdT38Vqt2o",
        "outputId": "81449429-a06a-462b-deb6-9d4f3258d3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+---------+\n",
            "|personId|lastName|firstName|\n",
            "+--------+--------+---------+\n",
            "|       1|    Wang|    Allen|\n",
            "|       2|   Alice|      Bob|\n",
            "+--------+--------+---------+\n",
            "\n",
            "+---------+--------+-------------+----------+\n",
            "|addressId|personId|         city|     state|\n",
            "+---------+--------+-------------+----------+\n",
            "|        1|       2|New York City|  New York|\n",
            "|        2|       3|     Leetcode|California|\n",
            "+---------+--------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = person_df.join(address_df, person_df[\"personId\"] == address_df[\"personId\"], \"left\")\n",
        "df_join.select('firstName', 'lastName','city','state').show()\n",
        "\n",
        "##first name, last name, city, and state"
      ],
      "metadata": {
        "id": "vutIvI1lrWwT",
        "outputId": "d1f3630e-2169-4fcd-a2b7-3ed660774d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------------+--------+\n",
            "|firstName|lastName|         city|   state|\n",
            "+---------+--------+-------------+--------+\n",
            "|    Allen|    Wang|         NULL|    NULL|\n",
            "|      Bob|   Alice|New York City|New York|\n",
            "+---------+--------+-------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "04: Employees Earning More Than Their Managers\n",
        "Write a Pyspark program to find Employees Earning More Than Their\n",
        "Managers"
      ],
      "metadata": {
        "id": "DtZVuMHessY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"employees\"\n",
        "employees_schema = StructType([\n",
        "  StructField(\"id\", IntegerType(), True),\n",
        "  StructField(\"name\", StringType(), True),\n",
        "  StructField(\"salary\", IntegerType(), True),\n",
        "  StructField(\"managerId\", IntegerType(), True)\n",
        "])\n",
        "# Define data for the \"employees\"\n",
        "employees_data = [\n",
        "  (1, 'Joe', 70000, 3),\n",
        "  (2, 'Henry', 80000, 4),\n",
        "  (3, 'Sam', 60000, None),\n",
        "  (4, 'Max', 90000, None)\n",
        "]\n",
        "\n",
        "emp_df = spark.createDataFrame(employees_data, employees_schema)\n",
        "emp_df.show()\n"
      ],
      "metadata": {
        "id": "SSTRUyDCr4KP",
        "outputId": "7307ea63-842b-4fa5-f7bc-22673f23d324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+---------+\n",
            "| id| name|salary|managerId|\n",
            "+---+-----+------+---------+\n",
            "|  1|  Joe| 70000|        3|\n",
            "|  2|Henry| 80000|        4|\n",
            "|  3|  Sam| 60000|     NULL|\n",
            "|  4|  Max| 90000|     NULL|\n",
            "+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df_1 = emp_df.alias('e1')\n",
        "emp_df_2 = emp_df.alias('e2')\n",
        "\n",
        "self_joined_df = emp_df_1.join(emp_df_2, col(\"e1.id\") == col(\"e2.managerId\"))\n",
        "self_joined_df.show()\n"
      ],
      "metadata": {
        "id": "ZUS5MtKctLWM",
        "outputId": "c5b00609-bd83-4b5e-c202-36173c489ff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+---------+---+-----+------+---------+\n",
            "| id|name|salary|managerId| id| name|salary|managerId|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "|  3| Sam| 60000|     NULL|  1|  Joe| 70000|        3|\n",
            "|  4| Max| 90000|     NULL|  2|Henry| 80000|        4|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df = self_joined_df.select(col(\"e2.name\"), col('e2.salary'), col(\"e1.salary\").alias(\"manager_salary\"))\n",
        "self_joined_after_df.show()"
      ],
      "metadata": {
        "id": "pAwgyERSuYrh",
        "outputId": "9d2161fa-0257-4baf-f7d8-d1d6a5832c3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------------+\n",
            "| name|salary|manager_salary|\n",
            "+-----+------+--------------+\n",
            "|  Joe| 70000|         60000|\n",
            "|Henry| 80000|         90000|\n",
            "+-----+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df.filter(self_joined_after_df.salary > self_joined_after_df.manager_salary).select('name').show()"
      ],
      "metadata": {
        "id": "ciBHiMBRvAB4",
        "outputId": "3045bfc2-6fde-4d49-9a08-2417d13d3f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| Joe|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Pyspark program to report all the duplicate emails.\n",
        "Note that it's guaranteed that the email field is not NULL."
      ],
      "metadata": {
        "id": "lxncLIOe0nlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"emails\" table\n",
        "emails_schema = StructType([\n",
        "StructField(\"id\", IntegerType(), True),\n",
        "StructField(\"email\", StringType(), True)\n",
        "])\n",
        "# Define data for the \"emails\" table\n",
        "emails_data = [\n",
        "(1, 'a@b.com'),\n",
        "(2, 'c@d.com'),\n",
        "(3, 'a@b.com')\n",
        "]\n",
        "\n",
        "# Create a PySpark DataFrame\n",
        "email_df = spark.createDataFrame(emails_data, emails_schema)\n",
        "email_df.show()\n"
      ],
      "metadata": {
        "id": "WupwVbyFvTA2",
        "outputId": "e9d401cc-5efd-433d-afe8-544e13c0c7a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|  email|\n",
            "+---+-------+\n",
            "|  1|a@b.com|\n",
            "|  2|c@d.com|\n",
            "|  3|a@b.com|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_email_group = email_df.groupBy('email').count()\n",
        "df_email_group.show()\n",
        "\n",
        "df_email_group.filter(df_email_group['count']>1).show()"
      ],
      "metadata": {
        "id": "yhSQ-MF-1On_",
        "outputId": "b7d2edb8-8119-485f-a16d-ff55e34f2260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "|c@d.com|    1|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to start page 21"
      ],
      "metadata": {
        "id": "WCakKP5s2AOy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JtWqBxS1Yp-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a program in pyspark last 3 days product sell rolling avarage price?"
      ],
      "metadata": {
        "id": "ga_2A6oe9hbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: product_id, date, price\n",
        "data = [\n",
        "    (\"p1\", \"2025-04-10\", 100),\n",
        "    (\"p1\", \"2025-04-11\", 110),\n",
        "    (\"p1\", \"2025-04-12\", 105),\n",
        "    (\"p1\", \"2025-04-13\", 120),\n",
        "    (\"p1\", \"2025-04-14\", 130),\n",
        "    (\"p2\", \"2025-04-10\", 200),\n",
        "    (\"p2\", \"2025-04-11\", 220),\n",
        "    (\"p2\", \"2025-04-12\", 210),\n",
        "    (\"p2\", \"2025-04-14\", 230),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"date\", \"price\"])\n",
        "# Convert date to DateType\n",
        "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Define window: partition by product_id, ordered by date, looking back 2 rows (3-day window)\n",
        "windowSpace = Window.partitionBy(\"product_id\").orderBy(\"date\").rowsBetween(-2, 0)\n",
        "\n",
        "# Add rolling average column\n",
        "df_with_rolling_avg_column = df.withColumn(\"rolling_avg_price\", avg(\"price\").over(windowSpace))\n",
        "\n",
        "df_with_rolling_avg_column.orderBy(\"product_id\", \"date\").show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUBpXEBm93vg",
        "outputId": "76f21cc4-0ab5-4d1d-c170-b30a5c506618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+\n",
            "|product_id|      date|price|\n",
            "+----------+----------+-----+\n",
            "|        p1|2025-04-10|  100|\n",
            "|        p1|2025-04-11|  110|\n",
            "|        p1|2025-04-12|  105|\n",
            "|        p1|2025-04-13|  120|\n",
            "|        p1|2025-04-14|  130|\n",
            "|        p2|2025-04-10|  200|\n",
            "|        p2|2025-04-11|  220|\n",
            "|        p2|2025-04-12|  210|\n",
            "|        p2|2025-04-14|  230|\n",
            "+----------+----------+-----+\n",
            "\n",
            "+----------+----------+-----+------------------+\n",
            "|product_id|      date|price| rolling_avg_price|\n",
            "+----------+----------+-----+------------------+\n",
            "|        p1|2025-04-10|  100|             100.0|\n",
            "|        p1|2025-04-11|  110|             105.0|\n",
            "|        p1|2025-04-12|  105|             105.0|\n",
            "|        p1|2025-04-13|  120|111.66666666666667|\n",
            "|        p1|2025-04-14|  130|118.33333333333333|\n",
            "|        p2|2025-04-10|  200|             200.0|\n",
            "|        p2|2025-04-11|  220|             210.0|\n",
            "|        p2|2025-04-12|  210|             210.0|\n",
            "|        p2|2025-04-14|  230|             220.0|\n",
            "+----------+----------+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re given a dataset containing employee names, departments, and their salaries.\n",
        "\n",
        " Your task is to rank the employees by salary within each department and classify them as:\n",
        "\n",
        "\"High Growth\" if the salary is higher than the previous person in that department\n",
        "\n",
        "\"Low Growth\" if the salary is lower\n",
        "\n",
        "\"No Growth\" if the salary is the same or it's the first employee in the department"
      ],
      "metadata": {
        "id": "EgPFWB8HyfBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Alice\", \"HR\", 50000),\n",
        "    (\"Bob\", \"IT\", 60000),\n",
        "    (\"Charlie\", \"HR\", 55000),\n",
        "    (\"David\", \"IT\", 70000),\n",
        "    (\"Eve\", \"HR\", 52000),\n",
        "    (\"Frank\", \"IT\", 65000),\n",
        "    (\"Grace\", \"HR\", 51000),\n",
        "    (\"Hank\", \"IT\", 68000),\n",
        "    (\"Ivy\", \"HR\", 53000),\n",
        "    (\"Jack\", \"IT\", 72000),\n",
        "    (\"Kelly\", \"HR\", 54000),\n",
        "    (\"Luke\", \"IT\", 69000),\n",
        "\n",
        "]\n",
        "columns = ['name', 'department', 'salary']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Defin window function and use LAG to compare the salary\n",
        "windowSpace = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "df_with_lag = df.withColumn(\"previous_salary\", lag(\"salary\").over(windowSpace))\n",
        "df_with_lag.show()\n",
        "\n",
        "#Apply Classification Logic\n",
        "final_df = df_with_lag.withColumn(\n",
        "    \"growth_status\",\n",
        "    when(col(\"previous_salary\").isNull(), \"No Growth\")\\\n",
        "    .when(col(\"salary\")>col(\"previous_salary\"), \"High Growth\")\\\n",
        "    .when(col(\"salary\")<col(\"previous_salary\"), \"Low Growth\")\\\n",
        "    .otherwise(\"No Growth\")\n",
        ")\n",
        "\n",
        "#Display the result\n",
        "final_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NOCxkBOM-AlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94608b50-a94b-4f06-a902-ae5561d55bb4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|   name|department|salary|\n",
            "+-------+----------+------+\n",
            "|  Alice|        HR| 50000|\n",
            "|    Bob|        IT| 60000|\n",
            "|Charlie|        HR| 55000|\n",
            "|  David|        IT| 70000|\n",
            "|    Eve|        HR| 52000|\n",
            "|  Frank|        IT| 65000|\n",
            "|  Grace|        HR| 51000|\n",
            "|   Hank|        IT| 68000|\n",
            "|    Ivy|        HR| 53000|\n",
            "|   Jack|        IT| 72000|\n",
            "|  Kelly|        HR| 54000|\n",
            "|   Luke|        IT| 69000|\n",
            "+-------+----------+------+\n",
            "\n",
            "+-------+----------+------+---------------+\n",
            "|   name|department|salary|previous_salary|\n",
            "+-------+----------+------+---------------+\n",
            "|  Alice|        HR| 50000|           NULL|\n",
            "|  Grace|        HR| 51000|          50000|\n",
            "|    Eve|        HR| 52000|          51000|\n",
            "|    Ivy|        HR| 53000|          52000|\n",
            "|  Kelly|        HR| 54000|          53000|\n",
            "|Charlie|        HR| 55000|          54000|\n",
            "|    Bob|        IT| 60000|           NULL|\n",
            "|  Frank|        IT| 65000|          60000|\n",
            "|   Hank|        IT| 68000|          65000|\n",
            "|   Luke|        IT| 69000|          68000|\n",
            "|  David|        IT| 70000|          69000|\n",
            "|   Jack|        IT| 72000|          70000|\n",
            "+-------+----------+------+---------------+\n",
            "\n",
            "+-------+----------+------+---------------+-------------+\n",
            "|   name|department|salary|previous_salary|growth_status|\n",
            "+-------+----------+------+---------------+-------------+\n",
            "|  Alice|        HR| 50000|           NULL|    No Growth|\n",
            "|  Grace|        HR| 51000|          50000|  High Growth|\n",
            "|    Eve|        HR| 52000|          51000|  High Growth|\n",
            "|    Ivy|        HR| 53000|          52000|  High Growth|\n",
            "|  Kelly|        HR| 54000|          53000|  High Growth|\n",
            "|Charlie|        HR| 55000|          54000|  High Growth|\n",
            "|    Bob|        IT| 60000|           NULL|    No Growth|\n",
            "|  Frank|        IT| 65000|          60000|  High Growth|\n",
            "|   Hank|        IT| 68000|          65000|  High Growth|\n",
            "|   Luke|        IT| 69000|          68000|  High Growth|\n",
            "|  David|        IT| 70000|          69000|  High Growth|\n",
            "|   Jack|        IT| 72000|          70000|  High Growth|\n",
            "+-------+----------+------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places. If a product does not have any sold units, its average selling price is assumed to be 0.\n"
      ],
      "metadata": {
        "id": "WbwcFFFqieVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "#Sample Data\n",
        "prices_data = [\n",
        "    (1, 100.0, \"2023-01-01\", \"2023-01-31\"),\n",
        "    (2, 200.0, \"2023-01-10\", \"2023-02-10\"),\n",
        "    (3, 150.0, \"2023-01-15\", \"2023-01-25\"),\n",
        "]\n",
        "\n",
        "units_sold_data =[\n",
        "    (1,\"2023-01-05\",10),\n",
        "    (2,\"2023-01-15\",5),\n",
        "    (2,\"2023-02-05\",5),\n",
        "    (4,\"2023-01-25\",5)\n",
        "]\n",
        "\n",
        "# Define Schema\n",
        "prices_column = ['product_id', 'price', 'start_date', 'end_date']\n",
        "units_sold_column = ['product_id', 'purchase_date', 'units']\n",
        "\n",
        "prices_df = spark.createDataFrame(prices_data, prices_column)\n",
        "units_sold_df = spark.createDataFrame(units_sold_data, units_sold_column)\n",
        "\n",
        "\n",
        "prices_df = prices_df.withColumn(\"start_data\", to_date(\"start_date\")).withColumn(\"end_date\", to_date(\"end_date\"))\n",
        "prices_df.show()\n",
        "\n",
        "units_sold_df = units_sold_df.withColumn(\"purchase_date\", to_date(\"purchase_date\"))\n",
        "units_sold_df.show()\n",
        "\n",
        "#Join wih date  filtering condition\n",
        "join_df = prices_df.join(units_sold_df, (prices_df.product_id == units_sold_df.product_id)&(units_sold_df.purchase_date.between(prices_df.start_date, prices_df.end_date)), \"left\")\n",
        "join_df.show()\n",
        "\n",
        "#Calculate total revenue and unit sold per product\n",
        "total_revenue_df = join_df.groupBy(\"product_id\")\\\n",
        "  .agg(\n",
        "      sum(col(\"price\") * col(\"units\")).alias(\"total_revenue\"),\n",
        "      sum(\"units\").alias(\"total_units\")\n",
        "    )\\\n",
        "  .withColumn(\"avarage_price\",\n",
        "              when(col(\"total_units\").isNull(), 0.0)\\\n",
        "              .otherwise(round(col(\"total-revenue\")/col(\"total_units\"), 2))\n",
        "                ).select(\"product_id\", \"avarage_price\")\n",
        "\n",
        "total_revenue_df.show()"
      ],
      "metadata": {
        "id": "uAimnEH1zIG4",
        "outputId": "497a632a-4e21-4559-fb08-04f9984bacfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+----------+----------+\n",
            "|product_id|price|start_date|  end_date|start_data|\n",
            "+----------+-----+----------+----------+----------+\n",
            "|         1|100.0|2023-01-01|2023-01-31|2023-01-01|\n",
            "|         2|200.0|2023-01-10|2023-02-10|2023-01-10|\n",
            "|         3|150.0|2023-01-15|2023-01-25|2023-01-15|\n",
            "+----------+-----+----------+----------+----------+\n",
            "\n",
            "+----------+-------------+-----+\n",
            "|product_id|purchase_date|units|\n",
            "+----------+-------------+-----+\n",
            "|         1|   2023-01-05|   10|\n",
            "|         2|   2023-01-15|    5|\n",
            "|         2|   2023-02-05|    5|\n",
            "|         4|   2023-01-25|    5|\n",
            "+----------+-------------+-----+\n",
            "\n",
            "+----------+-----+----------+----------+----------+----------+-------------+-----+\n",
            "|product_id|price|start_date|  end_date|start_data|product_id|purchase_date|units|\n",
            "+----------+-----+----------+----------+----------+----------+-------------+-----+\n",
            "|         1|100.0|2023-01-01|2023-01-31|2023-01-01|         1|   2023-01-05|   10|\n",
            "|         3|150.0|2023-01-15|2023-01-25|2023-01-15|      NULL|         NULL| NULL|\n",
            "|         2|200.0|2023-01-10|2023-02-10|2023-01-10|         2|   2023-02-05|    5|\n",
            "|         2|200.0|2023-01-10|2023-02-10|2023-01-10|         2|   2023-01-15|    5|\n",
            "+----------+-----+----------+----------+----------+----------+-------------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[AMBIGUOUS_REFERENCE] Reference `product_id` is ambiguous, could be: [`product_id`, `product_id`].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-eb98bc76ad00>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#Calculate total revenue and unit sold per product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtotal_revenue_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"product_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"product_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   .agg(\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"units\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_revenue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"units\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_units\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mexprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `product_id` is ambiguous, could be: [`product_id`, `product_id`]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hv7YbCG8pnvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}