{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Joins in Dataframe"
      ],
      "metadata": {
        "id": "WZtmuC6dtS5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part -1"
      ],
      "metadata": {
        "id": "Arg9jsaNtXqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Joins in PySpark: ***\n",
        "\n",
        "Joins are used to combine two DataFrames based on a common column or condition. PySpark supports several types of joins, similar to SQL. Below are explanations and examples for each type of join.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JOC2-avPtb8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Inner Join: (inner)**\n",
        "\n",
        "`inner_join = df1.join(df2, on=\"common_column\" how = \"inner\")`\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns rows where there is a match in both DataFrames (df1 and df2) based on the common_column.\n",
        "* Behavior: Rows with no matching value in either DataFrame are excluded.\n",
        "* Use Case: When you only need records that exist in both DataFrames."
      ],
      "metadata": {
        "id": "hAwCnTGdued8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Left Join (Left Outer Join) : (left)**\n",
        "\n",
        "`left_join = df1.join(df2, on=\"common_column\", how = \"left\")`\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns all rows from df1 and the matching rows from df2. If no match exists in df2, the result will contain NULL for columns from df2.\n",
        "* Behavior: All rows from the left DataFrame (df1) are preserved, even if there’s no match in the right DataFrame (df2).\n",
        "* Use Case: When you want to retain all rows from df1, even if there's no match in df2."
      ],
      "metadata": {
        "id": "fRgfsOZ6uz12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Right Join (Right Outer Join): (right)**\n",
        "\n",
        "` right_join = df1.join(df2, on=\"common_column\", how=\"right\") `\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns all rows from df2 and the matching rows from df1. If no match exists in df1, the result will contain NULL for columns from df1.\n",
        "* Behavior: All rows from the right DataFrame (df2) are preserved, even if there’s no match in the left DataFrame (df1).\n",
        "* Use Case: When you want to retain all rows from df2, even if there's no match in df1.\n"
      ],
      "metadata": {
        "id": "4MBz94E7uvV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Full Join (Outer Join) : (outer)**\n",
        "\n",
        "` full_join = df1.join(df2, on=\"common column\" how ='outer') `\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns all rows when there is a match in either df1 or df2. Non-matching rows will have NULL values in the columns from the other DataFrame.\n",
        "* Behavior: Retains all rows from both DataFrames, filling in NULL where there is no match.\n",
        "* Use Case: When you want to retain all rows from both DataFrames, regardless of whether there’s a match."
      ],
      "metadata": {
        "id": "93S1p_FYwBpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Left Semi Join (left_semi)**\n",
        "\n",
        "`left_semi_join = df1.join(df2, on=\"common_column\" how='left_semi')`\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns only the rows from df1 where there is a match in df2. It behaves like an inner join but only keeps columns from df1.\n",
        "* Behavior: Filters df1 to only keep rows that have a match in df2.\n",
        "* Use Case: When you want to filter df1 to keep rows with matching keys in df2, but you don’t need columns from df2.\n"
      ],
      "metadata": {
        "id": "8KbHareRwrSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Left Anti Join : (left_anti)**\n",
        "\n",
        "` left_anti_join = df1.join(df2, on='common column' how='left_anti') `\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns only the rows from df1 that do not have a match in df2.\n",
        "* Behavior: Filters out rows from df1 that have a match in df2.\n",
        "* Use Case: When you want to filter df1 to keep rows with no matching keys in df2\n"
      ],
      "metadata": {
        "id": "XNG_gTisxRxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Cross Join**\n",
        "\n",
        "` cross_join = df1.cross_join(df2) `\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns the Cartesian product of df1 and df2, meaning every row of df1 is paired with every row of df2.\n",
        "* Behavior: The number of rows in the result will be the product of the row count of df1 and df2.\n",
        "* Use Case: Typically used in edge cases or for generating combinations of rows, but be cautious as it can result in a very large DataFrame"
      ],
      "metadata": {
        "id": "d6iP39NxyeAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***8. Join with Explicit Conditions: (inner) ***\n",
        "\n",
        "`inner_join = df1.join(df2, (df1[\"columnA\"] == df2[\"columnB\"]), \"inner\")`\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: This is an example of an inner join where the common columns have different names in df1 and df2.\n",
        "* Behavior: Joins df1 and df2 based on a condition where columnA from df1 matches columnB from df2.\n",
        "* Use Case: When the join condition involves columns with different names or more complex conditions."
      ],
      "metadata": {
        "id": "6cUQjXEny25G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins Part 2"
      ],
      "metadata": {
        "id": "Hpx6C_tZz8yL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K9R9pyy1gU2Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate()\n",
        "# Sample DataFrames\n",
        "data1 = [Row(id=1), Row(id=1),  Row(id=None), Row(id=0)]\n",
        "data2 = [Row(id=1), Row(id=1), Row(id=1),  Row(id=None), Row(id=0)]\n",
        "df1 = spark.createDataFrame(data1)\n",
        "df2 = spark.createDataFrame(data2)\n",
        "df1.show()\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yTOjZlG0J5h",
        "outputId": "181ade2c-436c-4b1d-e314-aa2df1351dab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|NULL|\n",
            "|   0|\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|NULL|\n",
            "|   0|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inner Join\n",
        "inner_join = df1.join(df2, on=\"id\", how=\"inner\")\n",
        "\n",
        "print('Inner Join')\n",
        "inner_join.show()"
      ],
      "metadata": {
        "id": "ZhZ0wTu4uKTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e16ec4-e35d-4be7-e1bc-3b651abc02ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inner Join\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Left Join\n",
        "left_join = df1.join(df2, on='id', how='left')\n",
        "print('Left Join')\n",
        "left_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lLY-hNc1VbR",
        "outputId": "c81f70e1-289e-4b61-8bf7-b42f1589f08c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   0|\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Right Join\n",
        "right_join =df1.join(df2, on=\"id\", how='right')\n",
        "print('Right Jouin')\n",
        "right_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8phfav52rnt",
        "outputId": "7947e995-e117-4ee4-c3ef-338eb1891bf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right Jouin\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   0|\n",
            "|NULL|\n",
            "|   1|\n",
            "|   1|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Full Join\n",
        "full_join = df1.join(df2, on='id', how='outer')\n",
        "print('Full Join')\n",
        "full_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QLqkG5g2lUd",
        "outputId": "e4f5ac70-2f8a-4296-a27b-a9aa21c3f8c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|NULL|\n",
            "|NULL|\n",
            "|   0|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Left Anti Join\n",
        "left_anti_join = df1.join(df2, on='id', how='left_anti')\n",
        "print('Left Anti Join')\n",
        "left_anti_join.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csdjwZ-T3nMu",
        "outputId": "10a35b25-49cb-411e-fa60-3304ddc69228"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left Anti Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join)\n",
        "right_anti_join = df2.join(df1, on='id', how='left_anti')\n",
        "print('Right Anti Join')\n",
        "right_anti_join.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb4K1WcF4YcW",
        "outputId": "34feb52d-05f9-4d9b-d49f-2862a01e5770"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right Anti Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcast Join (Optimizing a join with a smaller DataFrame)\n",
        "broadcast_join = df1.join(broadcast(df2), on='id', how='inner')\n",
        "print('Broadcast Join')\n",
        "broadcast_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WaH8Ub_4zW2",
        "outputId": "ac00e06b-2be8-4091-fee6-698ac95d07b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcast Join\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  0|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Broadcast Join** is a performance optimization technique ideal for joining a large DataFrame with a small one efficiently, reducing shuffle costs."
      ],
      "metadata": {
        "id": "NJYt2ie35kgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins Part 3"
      ],
      "metadata": {
        "id": "IKgirJzw55ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_data = [\n",
        "    Row(emp_id=1, emp_name=\"Alice\", emp_salary=50000, emp_dept_id=101, emp_location=\"New York\"),\n",
        "    Row(emp_id=2, emp_name=\"Bob\", emp_salary=60000, emp_dept_id=102, emp_location=\"Los Angeles\"),\n",
        "    Row(emp_id=3, emp_name=\"Charlie\", emp_salary=55000, emp_dept_id=101, emp_location=\"Chicago\"),\n",
        "    Row(emp_id=4, emp_name=\"David\", emp_salary=70000, emp_dept_id=103, emp_location=\"San Francisco\"),\n",
        "    Row(emp_id=5, emp_name=\"Eve\", emp_salary=48000, emp_dept_id=102, emp_location=\"Houston\")\n",
        "  ]\n",
        "\n",
        "dept_data = [\n",
        "     Row(dept_id=101, dept_name=\"Engineering\", dept_head=\"John\", dept_location=\"New York\"),\n",
        "     Row(dept_id=102, dept_name=\"Marketing\", dept_head=\"Mary\", dept_location=\"Los Angeles\"),\n",
        "     Row(dept_id=103, dept_name=\"Finance\", dept_head=\"Frank\", dept_location=\"Chicago\"),\n",
        "     Row(dept_id=104, dept_name=\"Admin\", dept_head=\"Charlie\", dept_location=\"Houston\")\n",
        "\n",
        "    ]\n",
        "emp_columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_dept_id\", \"emp_location\"]\n",
        "dept_columns = [\"dept_id\", \"dept_name\", \"dept_head\", \"dept_location\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "emp_df.show()\n",
        "dept_df.show()"
      ],
      "metadata": {
        "id": "Ptrs_9Ub5boL",
        "outputId": "99aadc77-00ae-4abf-e4f9-dd9ff9506064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|\n",
            "|     3| Charlie|     55000|        101|      Chicago|\n",
            "|     4|   David|     70000|        103|San Francisco|\n",
            "|     5|     Eve|     48000|        102|      Houston|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n",
            "+-------+-----------+---------+-------------+\n",
            "|dept_id|  dept_name|dept_head|dept_location|\n",
            "+-------+-----------+---------+-------------+\n",
            "|    101|Engineering|     John|     New York|\n",
            "|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|    103|    Finance|    Frank|      Chicago|\n",
            "|    104|      Admin|  Charlie|      Houston|\n",
            "+-------+-----------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a PySpark query to find employees whose location matches the location of their department. Display emp_id, emp_name, emp_location, dept_name, and dept_location for matching records.\n",
        "Modify the code to find departments that"
      ],
      "metadata": {
        "id": "A2x4zZvhvcXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_location_df = emp_df.join(dept_df, emp_df['emp_location']==dept_df['dept_location'], \"inner\")\n",
        "emp_location_df.select('emp_id', 'emp_name', 'emp_location', 'dept_name', 'dept_location' ).show()"
      ],
      "metadata": {
        "id": "77JXQ0_ivhMS",
        "outputId": "164ac26f-d22d-49d2-ba20-32499f720952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_location|  dept_name|dept_location|\n",
            "+------+--------+------------+-----------+-------------+\n",
            "|     3| Charlie|     Chicago|    Finance|      Chicago|\n",
            "|     5|     Eve|     Houston|      Admin|      Houston|\n",
            "|     2|     Bob| Los Angeles|  Marketing|  Los Angeles|\n",
            "|     1|   Alice|    New York|Engineering|     New York|\n",
            "+------+--------+------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Modify the code to find departments that have no employees assigned to them. Display dept_id, dept_name, and dept_head.\n"
      ],
      "metadata": {
        "id": "m4Zwp7gAxM9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_department_df = dept_df.join(emp_df, emp_df['emp_dept_id'] == dept_df['dept_id'], 'left_anti' )\n",
        "no_department_df.show()"
      ],
      "metadata": {
        "id": "l6xECIrGxaCf",
        "outputId": "b9ed3bb5-f1a4-4d7c-a78f-e9912dfd4dd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-------------+\n",
            "|dept_id|dept_name|dept_head|dept_location|\n",
            "+-------+---------+---------+-------------+\n",
            "|    104|    Admin|  Charlie|      Houston|\n",
            "+-------+---------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a PySpark query to get the average salary of employees in each department, displaying dept_name and the calculated average_salary."
      ],
      "metadata": {
        "id": "GzDzrRrixO8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_salary_deparrt_df = emp_df.join(dept_df, emp_df['emp_dept_id']==dept_df['dept_id'], 'inner')\n",
        "average_salary_deparrt_df.show()\n",
        "average_salary_deparrt_df.groupBy(\"dept_name\").agg(avg(\"emp_salary\").alias(\"avarage_salary\")).select(\"dept_name\", \"avarage_salary\" ).show()"
      ],
      "metadata": {
        "id": "fPqm3Kw_xa50",
        "outputId": "4089a247-8715-471e-c065-8ebe70c86884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|dept_id|  dept_name|dept_head|dept_location|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|    101|Engineering|     John|     New York|\n",
            "|     3| Charlie|     55000|        101|      Chicago|    101|Engineering|     John|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     5|     Eve|     48000|        102|      Houston|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     4|   David|     70000|        103|San Francisco|    103|    Finance|    Frank|      Chicago|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "\n",
            "+-----------+--------------+\n",
            "|  dept_name|avarage_salary|\n",
            "+-----------+--------------+\n",
            "|Engineering|       52500.0|\n",
            "|    Finance|       70000.0|\n",
            "|  Marketing|       54000.0|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List the employees who earn more than the average salary of their department. Display emp_id, emp_name, emp_salary, dept_name, and dept_location."
      ],
      "metadata": {
        "id": "-EiVh6p4xVGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dep_wise_emp_df = emp_df.join(dept_df, emp_df['emp_dept_id'] == dept_df['dept_id'], \"inner\")\n",
        "dep_wise_emp_df.show()\n",
        "\n",
        "avarage_salary = dep_wise_emp_df.groupBy(\"dept_id\").agg(avg(\"emp_salary\").alias(\"avarage_salary\"))\n",
        "avarage_salary.show()\n",
        "\n",
        "get_greater_great_avg_salary_emp_df = dep_wise_emp_df.join(avarage_salary,\"dept_id\", \"inner\" )\n",
        "get_greater_great_avg_salary_emp_df.show()\n",
        "\n",
        "get_greater_great_avg_salary_emp_df.select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\").filter('emp_salary>avarage_salary').show()\n"
      ],
      "metadata": {
        "id": "7fW7e9PmwvML",
        "outputId": "fb2a2752-6a69-4cbe-da4f-dc3a26593696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|dept_id|  dept_name|dept_head|dept_location|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|    101|Engineering|     John|     New York|\n",
            "|     3| Charlie|     55000|        101|      Chicago|    101|Engineering|     John|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     5|     Eve|     48000|        102|      Houston|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     4|   David|     70000|        103|San Francisco|    103|    Finance|    Frank|      Chicago|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "\n",
            "+-------+--------------+\n",
            "|dept_id|avarage_salary|\n",
            "+-------+--------------+\n",
            "|    101|       52500.0|\n",
            "|    102|       54000.0|\n",
            "|    103|       70000.0|\n",
            "+-------+--------------+\n",
            "\n",
            "+-------+------+--------+----------+-----------+-------------+-----------+---------+-------------+--------------+\n",
            "|dept_id|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|  dept_name|dept_head|dept_location|avarage_salary|\n",
            "+-------+------+--------+----------+-----------+-------------+-----------+---------+-------------+--------------+\n",
            "|    101|     3| Charlie|     55000|        101|      Chicago|Engineering|     John|     New York|       52500.0|\n",
            "|    101|     1|   Alice|     50000|        101|     New York|Engineering|     John|     New York|       52500.0|\n",
            "|    102|     5|     Eve|     48000|        102|      Houston|  Marketing|     Mary|  Los Angeles|       54000.0|\n",
            "|    102|     2|     Bob|     60000|        102|  Los Angeles|  Marketing|     Mary|  Los Angeles|       54000.0|\n",
            "|    103|     4|   David|     70000|        103|San Francisco|    Finance|    Frank|      Chicago|       70000.0|\n",
            "+-------+------+--------+----------+-----------+-------------+-----------+---------+-------------+--------------+\n",
            "\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|  dept_name|dept_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     3| Charlie|     55000|Engineering|     New York|\n",
            "|     2|     Bob|     60000|  Marketing|  Los Angeles|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Join Employee and Department Data\n",
        "dep_wise_emp_df = emp_df.join(dept_df, emp_df['emp_dept_id'] == dept_df['dept_id'], \"inner\")\n",
        "\n",
        "# Compute Average Salary per Department\n",
        "average_salary_df = dep_wise_emp_df.groupBy(\"dept_id\").agg(avg(\"emp_salary\").alias(\"avg_salary\"))\n",
        "\n",
        "# Join with Employees to Get Only Those Who Earn More Than the Department Average\n",
        "high_salary_emp_df = dep_wise_emp_df.join(average_salary_df, \"dept_id\") \\\n",
        "    .filter(dep_wise_emp_df.emp_salary > average_salary_df.avg_salary) \\\n",
        "    .select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\")\n",
        "\n",
        "# Show Result\n",
        "high_salary_emp_df.show()\n"
      ],
      "metadata": {
        "id": "RtkwBbUD5DZh",
        "outputId": "dd579297-76e5-4524-a921-d6720f7ff6dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|  dept_name|dept_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     3| Charlie|     55000|Engineering|     New York|\n",
            "|     2|     Bob|     60000|  Marketing|  Los Angeles|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBEV0LxL-LmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}