{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Joins in Dataframe"
      ],
      "metadata": {
        "id": "WZtmuC6dtS5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part -1"
      ],
      "metadata": {
        "id": "Arg9jsaNtXqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Joins in PySpark: ***\n",
        "\n",
        "Joins are used to combine two DataFrames based on a common column or condition. PySpark supports several types of joins, similar to SQL. Below are explanations and examples for each type of join.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JOC2-avPtb8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Inner Join: (inner)**\n",
        "\n",
        "`inner_join = df1.join(df2, on=\"common_column\" how = \"inner\")`\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns rows where there is a match in both DataFrames (df1 and df2) based on the common_column.\n",
        "* Behavior: Rows with no matching value in either DataFrame are excluded.\n",
        "* Use Case: When you only need records that exist in both DataFrames."
      ],
      "metadata": {
        "id": "hAwCnTGdued8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Left Join (Left Outer Join) : (left)**\n",
        "\n",
        "`left_join = df1.join(df2, on=\"common_column\", how = \"left\")`\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns all rows from df1 and the matching rows from df2. If no match exists in df2, the result will contain NULL for columns from df2.\n",
        "* Behavior: All rows from the left DataFrame (df1) are preserved, even if there’s no match in the right DataFrame (df2).\n",
        "* Use Case: When you want to retain all rows from df1, even if there's no match in df2."
      ],
      "metadata": {
        "id": "fRgfsOZ6uz12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Right Join (Right Outer Join): (right)**\n",
        "\n",
        "` right_join = df1.join(df2, on=\"common_column\", how=\"right\") `\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns all rows from df2 and the matching rows from df1. If no match exists in df1, the result will contain NULL for columns from df1.\n",
        "* Behavior: All rows from the right DataFrame (df2) are preserved, even if there’s no match in the left DataFrame (df1).\n",
        "* Use Case: When you want to retain all rows from df2, even if there's no match in df1.\n"
      ],
      "metadata": {
        "id": "4MBz94E7uvV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Full Join (Outer Join) : (outer)**\n",
        "\n",
        "` full_join = df1.join(df2, on=\"common column\" how ='outer') `\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns all rows when there is a match in either df1 or df2. Non-matching rows will have NULL values in the columns from the other DataFrame.\n",
        "* Behavior: Retains all rows from both DataFrames, filling in NULL where there is no match.\n",
        "* Use Case: When you want to retain all rows from both DataFrames, regardless of whether there’s a match."
      ],
      "metadata": {
        "id": "93S1p_FYwBpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Left Semi Join (left_semi)**\n",
        "\n",
        "`left_semi_join = df1.join(df2, on=\"common_column\" how='left_semi')`\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns only the rows from df1 where there is a match in df2. It behaves like an inner join but only keeps columns from df1.\n",
        "* Behavior: Filters df1 to only keep rows that have a match in df2.\n",
        "* Use Case: When you want to filter df1 to keep rows with matching keys in df2, but you don’t need columns from df2.\n"
      ],
      "metadata": {
        "id": "8KbHareRwrSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Left Anti Join : (left_anti)**\n",
        "\n",
        "` left_anti_join = df1.join(df2, on='common column' how='left_anti') `\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns only the rows from df1 that do not have a match in df2.\n",
        "* Behavior: Filters out rows from df1 that have a match in df2.\n",
        "* Use Case: When you want to filter df1 to keep rows with no matching keys in df2\n"
      ],
      "metadata": {
        "id": "XNG_gTisxRxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Cross Join**\n",
        "\n",
        "` cross_join = df1.cross_join(df2) `\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: Returns the Cartesian product of df1 and df2, meaning every row of df1 is paired with every row of df2.\n",
        "* Behavior: The number of rows in the result will be the product of the row count of df1 and df2.\n",
        "* Use Case: Typically used in edge cases or for generating combinations of rows, but be cautious as it can result in a very large DataFrame"
      ],
      "metadata": {
        "id": "d6iP39NxyeAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***8. Join with Explicit Conditions: (inner) ***\n",
        "\n",
        "`inner_join = df1.join(df2, (df1[\"columnA\"] == df2[\"columnB\"]), \"inner\")`\n",
        "\n",
        "**Explanation:**\n",
        "* Purpose: This is an example of an inner join where the common columns have different names in df1 and df2.\n",
        "* Behavior: Joins df1 and df2 based on a condition where columnA from df1 matches columnB from df2.\n",
        "* Use Case: When the join condition involves columns with different names or more complex conditions."
      ],
      "metadata": {
        "id": "6cUQjXEny25G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins Part 2"
      ],
      "metadata": {
        "id": "Hpx6C_tZz8yL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "K9R9pyy1gU2Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"JoinsExample\").getOrCreate()\n",
        "# Sample DataFrames\n",
        "data1 = [Row(id=1), Row(id=1),  Row(id=None), Row(id=0)]\n",
        "data2 = [Row(id=1), Row(id=1), Row(id=1),  Row(id=None), Row(id=0)]\n",
        "df1 = spark.createDataFrame(data1)\n",
        "df2 = spark.createDataFrame(data2)\n",
        "df1.show()\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yTOjZlG0J5h",
        "outputId": "0987b2ee-0e50-466a-8c46-84ff058477f4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|NULL|\n",
            "|   0|\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|NULL|\n",
            "|   0|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inner Join\n",
        "inner_join = df1.join(df2, on=\"id\", how=\"inner\")\n",
        "\n",
        "print('Inner Join')\n",
        "inner_join.show()"
      ],
      "metadata": {
        "id": "ZhZ0wTu4uKTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa003a2-8621-419d-cfb2-e412d7f4b1ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inner Join\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Left Join\n",
        "left_join = df1.join(df2, on='id', how='left')\n",
        "print('Left Join')\n",
        "left_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lLY-hNc1VbR",
        "outputId": "ecc68034-3f00-4963-f40c-60c6ef2ebea0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   0|\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Right Join\n",
        "right_join =df1.join(df2, on=\"id\", how='right')\n",
        "print('Right Jouin')\n",
        "right_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8phfav52rnt",
        "outputId": "9d81fa45-a733-4119-87d9-14167cf1585f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right Jouin\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   0|\n",
            "|NULL|\n",
            "|   1|\n",
            "|   1|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Full Join\n",
        "full_join = df1.join(df2, on='id', how='outer')\n",
        "print('Full Join')\n",
        "full_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QLqkG5g2lUd",
        "outputId": "6a156c85-1590-4495-e3fa-44a7efe333b3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|NULL|\n",
            "|NULL|\n",
            "|   0|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "|   1|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Left Anti Join\n",
        "left_anti_join = df1.join(df2, on='id', how='left_anti')\n",
        "print('Left Anti Join')\n",
        "left_anti_join.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csdjwZ-T3nMu",
        "outputId": "f563e7dc-0732-4271-878f-015aa8c52d52"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left Anti Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Right Anti Join (Equivalent to swapping DataFrames and performing Left Anti Join)\n",
        "right_anti_join = df2.join(df1, on='id', how='left_anti')\n",
        "print('Right Anti Join')\n",
        "right_anti_join.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb4K1WcF4YcW",
        "outputId": "bb2a5622-d78d-48e1-972f-157a9d2edb44"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right Anti Join\n",
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcast Join (Optimizing a join with a smaller DataFrame)\n",
        "broadcast_join = df1.join(broadcast(df2), on='id', how='inner')\n",
        "print('Broadcast Join')\n",
        "broadcast_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WaH8Ub_4zW2",
        "outputId": "fcb8a7a4-27ff-458d-87f7-0e5ca6775546"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcast Join\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  1|\n",
            "|  0|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Broadcast Join** is a performance optimization technique ideal for joining a large DataFrame with a small one efficiently, reducing shuffle costs."
      ],
      "metadata": {
        "id": "NJYt2ie35kgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins Part 3"
      ],
      "metadata": {
        "id": "IKgirJzw55ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_data = [\n",
        "    Row(emp_id=1, emp_name=\"Alice\", emp_salary=50000, emp_dept_id=101, emp_location=\"New York\"),\n",
        "    Row(emp_id=2, emp_name=\"Bob\", emp_salary=60000, emp_dept_id=102, emp_location=\"Los Angeles\"),\n",
        "    Row(emp_id=3, emp_name=\"Charlie\", emp_salary=55000, emp_dept_id=101, emp_location=\"Chicago\"),\n",
        "    Row(emp_id=4, emp_name=\"David\", emp_salary=70000, emp_dept_id=103, emp_location=\"San Francisco\"),\n",
        "    Row(emp_id=5, emp_name=\"Eve\", emp_salary=48000, emp_dept_id=102, emp_location=\"Houston\")\n",
        "  ]\n",
        "\n",
        "dept_data = [\n",
        "     Row(dept_id=101, dept_name=\"Engineering\", dept_head=\"John\", dept_location=\"New York\"),\n",
        "     Row(dept_id=102, dept_name=\"Marketing\", dept_head=\"Mary\", dept_location=\"Los Angeles\"),\n",
        "     Row(dept_id=103, dept_name=\"Finance\", dept_head=\"Frank\", dept_location=\"Chicago\"),\n",
        "     Row(dept_id=104, dept_name=\"Admin\", dept_head=\"Charlie\", dept_location=\"Houston\")\n",
        "\n",
        "    ]\n",
        "emp_columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_dept_id\", \"emp_location\"]\n",
        "dept_columns = [\"dept_id\", \"dept_name\", \"dept_head\", \"dept_location\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "emp_df.show()\n",
        "dept_df.show()"
      ],
      "metadata": {
        "id": "Ptrs_9Ub5boL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1aeb5ee-75c8-4657-d04c-8ba781a5f6a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|\n",
            "|     3| Charlie|     55000|        101|      Chicago|\n",
            "|     4|   David|     70000|        103|San Francisco|\n",
            "|     5|     Eve|     48000|        102|      Houston|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n",
            "+-------+-----------+---------+-------------+\n",
            "|dept_id|  dept_name|dept_head|dept_location|\n",
            "+-------+-----------+---------+-------------+\n",
            "|    101|Engineering|     John|     New York|\n",
            "|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|    103|    Finance|    Frank|      Chicago|\n",
            "|    104|      Admin|  Charlie|      Houston|\n",
            "+-------+-----------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a PySpark query to find employees whose location matches the location of their department. Display emp_id, emp_name, emp_location, dept_name, and dept_location for matching records.\n",
        "Modify the code to find departments that"
      ],
      "metadata": {
        "id": "A2x4zZvhvcXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_location_df = emp_df.join(dept_df, emp_df['emp_location']==dept_df['dept_location'], \"inner\")\n",
        "emp_location_df.select('emp_id', 'emp_name', 'emp_location', 'dept_name', 'dept_location' ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77JXQ0_ivhMS",
        "outputId": "468fa3fb-5264-41f1-900b-0cc62bbf17fa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_location|  dept_name|dept_location|\n",
            "+------+--------+------------+-----------+-------------+\n",
            "|     3| Charlie|     Chicago|    Finance|      Chicago|\n",
            "|     5|     Eve|     Houston|      Admin|      Houston|\n",
            "|     2|     Bob| Los Angeles|  Marketing|  Los Angeles|\n",
            "|     1|   Alice|    New York|Engineering|     New York|\n",
            "+------+--------+------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Modify the code to find departments that have no employees assigned to them. Display dept_id, dept_name, and dept_head.\n"
      ],
      "metadata": {
        "id": "m4Zwp7gAxM9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_department_df = dept_df.join(emp_df, emp_df['emp_dept_id'] == dept_df['dept_id'], 'left_anti' )\n",
        "no_department_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6xECIrGxaCf",
        "outputId": "7997a1f0-91db-40e0-aad2-ca0d610787f1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+-------------+\n",
            "|dept_id|dept_name|dept_head|dept_location|\n",
            "+-------+---------+---------+-------------+\n",
            "|    104|    Admin|  Charlie|      Houston|\n",
            "+-------+---------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a PySpark query to get the average salary of employees in each department, displaying dept_name and the calculated average_salary."
      ],
      "metadata": {
        "id": "GzDzrRrixO8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_salary_deparrt_df = emp_df.join(dept_df, emp_df['emp_dept_id']==dept_df['dept_id'], 'inner')\n",
        "average_salary_deparrt_df.show()\n",
        "average_salary_deparrt_df.groupBy(\"dept_name\").agg(avg(\"emp_salary\").alias(\"avarage_salary\")).select(\"dept_name\", \"avarage_salary\" ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPqm3Kw_xa50",
        "outputId": "e6cf483c-bd6a-4d6c-c20e-8bbe6d706403"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|dept_id|  dept_name|dept_head|dept_location|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|    101|Engineering|     John|     New York|\n",
            "|     3| Charlie|     55000|        101|      Chicago|    101|Engineering|     John|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     5|     Eve|     48000|        102|      Houston|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     4|   David|     70000|        103|San Francisco|    103|    Finance|    Frank|      Chicago|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "\n",
            "+-----------+--------------+\n",
            "|  dept_name|avarage_salary|\n",
            "+-----------+--------------+\n",
            "|Engineering|       52500.0|\n",
            "|    Finance|       70000.0|\n",
            "|  Marketing|       54000.0|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List the employees who earn more than the average salary of their department. Display emp_id, emp_name, emp_salary, dept_name, and dept_location."
      ],
      "metadata": {
        "id": "-EiVh6p4xVGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dep_wise_emp_df = emp_df.join(dept_df, emp_df['emp_dept_id'] == dept_df['dept_id'], \"inner\")\n",
        "dep_wise_emp_df.show()\n",
        "\n",
        "avarage_salary = dep_wise_emp_df.groupBy(\"dept_id\").agg(avg(\"emp_salary\").alias(\"avarage_salary\"))\n",
        "avarage_salary.show()\n",
        "\n",
        "get_greater_great_avg_salary_emp_df = dep_wise_emp_df.\\\n",
        "  join(avarage_salary,\"dept_id\", \"inner\" )\n",
        "\n",
        "get_greater_great_avg_salary_emp_df.show()\n",
        "\n",
        "get_greater_great_avg_salary_emp_df.\\\n",
        "  select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\").\\\n",
        "    filter('emp_salary>avarage_salary').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fW7e9PmwvML",
        "outputId": "116be436-0120-4452-ba83-fb336fd54ad7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|dept_id|  dept_name|dept_head|dept_location|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|    101|Engineering|     John|     New York|\n",
            "|     3| Charlie|     55000|        101|      Chicago|    101|Engineering|     John|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     5|     Eve|     48000|        102|      Houston|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     4|   David|     70000|        103|San Francisco|    103|    Finance|    Frank|      Chicago|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "\n",
            "+-------+--------------+\n",
            "|dept_id|avarage_salary|\n",
            "+-------+--------------+\n",
            "|    101|       52500.0|\n",
            "|    102|       54000.0|\n",
            "|    103|       70000.0|\n",
            "+-------+--------------+\n",
            "\n",
            "+-------+------+--------+----------+-----------+-------------+-----------+---------+-------------+--------------+\n",
            "|dept_id|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|  dept_name|dept_head|dept_location|avarage_salary|\n",
            "+-------+------+--------+----------+-----------+-------------+-----------+---------+-------------+--------------+\n",
            "|    101|     3| Charlie|     55000|        101|      Chicago|Engineering|     John|     New York|       52500.0|\n",
            "|    101|     1|   Alice|     50000|        101|     New York|Engineering|     John|     New York|       52500.0|\n",
            "|    102|     5|     Eve|     48000|        102|      Houston|  Marketing|     Mary|  Los Angeles|       54000.0|\n",
            "|    102|     2|     Bob|     60000|        102|  Los Angeles|  Marketing|     Mary|  Los Angeles|       54000.0|\n",
            "|    103|     4|   David|     70000|        103|San Francisco|    Finance|    Frank|      Chicago|       70000.0|\n",
            "+-------+------+--------+----------+-----------+-------------+-----------+---------+-------------+--------------+\n",
            "\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|  dept_name|dept_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     3| Charlie|     55000|Engineering|     New York|\n",
            "|     2|     Bob|     60000|  Marketing|  Los Angeles|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Join Employee and Department Data\n",
        "dep_wise_emp_df = emp_df.join(dept_df, emp_df['emp_dept_id'] == dept_df['dept_id'], \"inner\")\n",
        "\n",
        "# Compute Average Salary per Department\n",
        "average_salary_df = dep_wise_emp_df.groupBy(\"dept_id\").agg(avg(\"emp_salary\").alias(\"avg_salary\"))\n",
        "\n",
        "# Join with Employees to Get Only Those Who Earn More Than the Department Average\n",
        "high_salary_emp_df = dep_wise_emp_df.join(average_salary_df, \"dept_id\") \\\n",
        "    .filter(dep_wise_emp_df.emp_salary > average_salary_df.avg_salary) \\\n",
        "    .select(\"emp_id\", \"emp_name\", \"emp_salary\", \"dept_name\", \"dept_location\")\n",
        "\n",
        "# Show Result\n",
        "high_salary_emp_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtkwBbUD5DZh",
        "outputId": "9737a07f-2a1e-449c-c403-39028907af6d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|  dept_name|dept_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     3| Charlie|     55000|Engineering|     New York|\n",
            "|     2|     Bob|     60000|  Marketing|  Los Angeles|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joins Part 3"
      ],
      "metadata": {
        "id": "K7kbGTgO34rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a PySpark query to create a DataFrame that lists each employee along with their manager's name. Display columns employee and manager.\n"
      ],
      "metadata": {
        "id": "_mUsqMav5Ghg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate()\n",
        "\n",
        "#Sample Data\n",
        "data = [\n",
        "      (1, None, \"CEO\"),\n",
        "      (2, 1, \"Manager A\"),\n",
        "      (3, 1, \"Manager B\"),\n",
        "      (4, 2, \"Employee X\"),\n",
        "      (5, 3, \"Employee Y\")\n",
        "      ]\n",
        "columns = [\"empid\", \"mrgid\", \"ename\"]\n",
        "\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "employee_df.show()"
      ],
      "metadata": {
        "id": "mBEV0LxL-LmA",
        "outputId": "591704dd-87b0-4d0c-eed5-072ff31ed2d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+\n",
            "|empid|mrgid|     ename|\n",
            "+-----+-----+----------+\n",
            "|    1| NULL|       CEO|\n",
            "|    2|    1| Manager A|\n",
            "|    3|    1| Manager B|\n",
            "|    4|    2|Employee X|\n",
            "|    5|    3|Employee Y|\n",
            "+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Modify the code to find and display only the employee(s) who do not have a manager (CEO-level employees). Display columns employee and manager.\n"
      ],
      "metadata": {
        "id": "yA1OV8sf5OiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manager_df = employee_df.alias('e').join(employee_df.alias('m'), col(\"e.mrgid\") == col(\"m.mrgid\"), \"left\" ).select(col(\"e.ename\").alias(\"employe\"), col(\"m.ename\").alias(\"manager\"))\n",
        "manager_df.show()"
      ],
      "metadata": {
        "id": "uIytEjzN5U6z",
        "outputId": "e360f971-e783-4ce0-d698-9f7590d84aeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|   employe|   manager|\n",
            "+----------+----------+\n",
            "|       CEO|      NULL|\n",
            "| Manager A| Manager B|\n",
            "| Manager A| Manager A|\n",
            "| Manager B| Manager B|\n",
            "| Manager B| Manager A|\n",
            "|Employee Y|Employee Y|\n",
            "|Employee X|Employee X|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Extend the code to find all employees who directly report to \"Manager A.\" Display columns empid, ename, and mrgid.\n"
      ],
      "metadata": {
        "id": "WoxPWrrs5Qd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manager_df_2 = employee_df.alias(\"e1\")\\\n",
        "  .join(employee_df.alias(\"m1\"), col(\"e1.mrgid\") == col(\"m1.empid\"), \"left\")\\\n",
        "  .select(col(\"e1.ename\").alias(\"employee\"), col(\"m1.ename\").alias(\"manager\"))\\\n",
        "  .filter(col(\"manager\").isNotNull())\n",
        "\n",
        "manager_df_2.show()"
      ],
      "metadata": {
        "id": "Bj-ZXgxd5VQv",
        "outputId": "28e8b6fe-1ddc-42ce-9bb3-c93cfbeeab05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|  employee|  manager|\n",
            "+----------+---------+\n",
            "| Manager A|      CEO|\n",
            "| Manager B|      CEO|\n",
            "|Employee X|Manager A|\n",
            "|Employee Y|Manager B|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a query to determine the hierarchy level of each employee, where the CEO is level 1, direct reports to the CEO are level 2, and so on. Display columns empid, ename, mrgid, and level."
      ],
      "metadata": {
        "id": "4aqUnfNf5SPY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6hWT_oN4E_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}