{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "K9R9pyy1gU2Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive\n",
        "from os import truncate\n",
        "# Mount Google Drive with a longer timeout\n",
        "# drive.mount('/content/drive', force_remount=True, timeout_ms=300000)\n",
        "\n",
        "# df_employee_data = \"/content/drive/MyDrive/Colab Notebooks/dataSet/employee_data.csv\"\n",
        "# employeeSechema = StructType([\n",
        "#     StructField(\"ID\",IntegerType() ,True),\n",
        "#     StructField(\"Name\",StringType() ,True),\n",
        "#     StructField(\"Age\",IntegerType() ,True),\n",
        "#     StructField(\"Salary\",FloatType() ,True),\n",
        "#     StructField(\"Joining_Date\",DateType() ,True),\n",
        "#     StructField(\"Department\",StringType() ,True),\n",
        "#     StructField(\"Performance_Rating\",IntegerType() ,True),\n",
        "#     StructField(\"Email\",StringType() ,True),\n",
        "#     StructField(\"Address\",StringType() ,True),\n",
        "#     StructField(\"Phone\",StringType() ,True)\n",
        "\n",
        "# ])\n",
        "# # Load the DataFrame with the defined schema\n",
        "# #df = spark.read.csv(path=df_employee_data, header=True, schema=employeeSechema)\n",
        "# df = spark.read.load(path=\"/content/drive/MyDrive/Colab Notebooks/dataSet/employee_data.csv\", format=\"csv\", header = True, schema=employeeSechema)\n",
        "# df.printSchema()\n",
        "# df.show(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Date Function in Dataframe – Part 1"
      ],
      "metadata": {
        "id": "YVFhJ0j8ZxER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation with Notes\n",
        "1. Creating a Spark Session:\n",
        "  * We begin by creating a Spark session to run the PySpark operations.\n",
        "2. Generating a DataFrame:\n",
        "  * Using spark.range(10) creates a DataFrame with 10 rows and a single column (id) with numbers ranging from 0 to 9.\n",
        "  * Two additional columns are added:\n",
        "    * today: Contains the current date using current_date().\n",
        "    * now: Contains the current timestamp using current_timestamp().\n",
        "3. Date Manipulation Functions:\n",
        "  * date_add: Adds a specified number of days to the date.\n",
        "  * date_sub: Subtracts a specified number of days from the date.\n",
        "  * datediff: Returns the difference in days between two dates.\n",
        "  * months_between: Returns the number of months between two dates.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SMGwfMlKZ4_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateDf = spark.range(10).withColumn(\"Today\", current_date()).withColumn(\"Now\", current_timestamp())\n",
        "dateDf.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebQZ3XkwZzfc",
        "outputId": "5263d809-5877-4054-f8b9-c11dab71c251"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------------+\n",
            "|id |Today     |Now                       |\n",
            "+---+----------+--------------------------+\n",
            "|0  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|1  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|2  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|3  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|4  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|5  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|6  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|7  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|8  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "|9  |2025-03-27|2025-03-27 17:22:59.714692|\n",
            "+---+----------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. date_add and date_sub:"
      ],
      "metadata": {
        "id": "dBi16uQqbf06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateDf.select(\n",
        "    date_sub(col(\"Today\"), 5).alias(\"date_sub_5_days\"),\n",
        "    date_add(col(\"Today\"), 5).alias(\"date_add_5_days\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDYvMBoJb3_a",
        "outputId": "ad8b3783-0182-4bb4-e008-a2b7070fe8b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+\n",
            "|date_sub_5_days|date_add_5_days|\n",
            "+---------------+---------------+\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Datediff:\n",
        "  * datediff(col(\"week_ago\"), col(\"today\")): Calculates the difference in days between the current date and 7 days ago (i.e., -7).\n"
      ],
      "metadata": {
        "id": "ONCNIRB4c3Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the days diffrence between \"today\" and \"week_ago\" (7 days ago)\n",
        "dateDf.withColumn(\"week_ago\", date_sub(col(\"today\"),7)).select(datediff(col(\"week_ago\"), col(\"today\")).alias(\"days_difference\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCwyVCqJckic",
        "outputId": "80906c16-4f48-4333-cb2a-53d874bf73f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|days_difference|\n",
            "+---------------+\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. months_between:\n",
        "* months_between(to_date(lit(\"2016-01-01\")), to_date(lit(\"2017-01-01\")): Calculates the number of months between January 1, 2016, and January 1, 2017, which is -12 months because start_date is earlier than end_date."
      ],
      "metadata": {
        "id": "w70odkFah47k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of months between two specific dates\n",
        "dateDf.select(\n",
        "    to_date(lit(\"2016-01-01\")).alias(\"start_date\"),\n",
        "    to_date(lit(\"2017-01-01\")).alias(\"end_date\")\n",
        ").select(months_between(col(\"start_date\"),col(\"end_date\")).alias(\"months_between\")).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUxDyv1dhvT_",
        "outputId": "a83d81f4-f8f6-4f26-c54b-d14810d0d00c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|months_between|\n",
            "+--------------+\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Date Function in Dataframe – Part 2"
      ],
      "metadata": {
        "id": "zMsrWEKOjRPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Default Date Parsing (to_date):\n",
        "  * When using to_date(), the default date format is yyyy-MM-dd.\n",
        "  * If the format of the string does not match this, PySpark returns null for invalid date parsing."
      ],
      "metadata": {
        "id": "jVqu8w8CjYei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateDf.select(\n",
        "    to_date(lit(\"2016-20-12\")).alias(\"incorrect_date\"),\n",
        "    to_date(lit(\"2025-12-11\")).alias(\"correct_date\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YUvqviqjH6p",
        "outputId": "90ede5a0-986d-4a19-8416-4e104c163a98"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+\n",
            "|incorrect_date|correct_date|\n",
            "+--------------+------------+\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Custom Date Formats:\n",
        "  * You can specify a custom date format using the to_date function by providing a format string, such as yyyy-dd-MM.\n",
        "  * This allows PySpark to correctly parse the dates that deviate from the default format."
      ],
      "metadata": {
        "id": "8wcKrHGOkTw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateFormat = \"yyyy-dd-MM\"\n",
        "cleanDateDf = spark.range(5).select(\n",
        "    to_date(lit(\"2023-12-11\"), dateFormat).alias(\"correct_date_format\"),\n",
        "    to_date(lit(\"2023-20-13\"), dateFormat).alias(\"incorrect_date_format\"),\n",
        ")\n",
        "\n",
        "cleanDateDf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRFcDxejkIaT",
        "outputId": "f55a3bcd-e69b-4001-c248-63789eac24e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------------------+\n",
            "|correct_date_format|incorrect_date_format|\n",
            "+-------------------+---------------------+\n",
            "|         2023-11-12|                 NULL|\n",
            "|         2023-11-12|                 NULL|\n",
            "|         2023-11-12|                 NULL|\n",
            "|         2023-11-12|                 NULL|\n",
            "|         2023-11-12|                 NULL|\n",
            "+-------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Handling Timestamps:"
      ],
      "metadata": {
        "id": "E8a6v-Bxl_9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A075MC-woUML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleanDateDf.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHhCAvFjpCXv",
        "outputId": "bb085eee-3485-4197-c66e-1bd9688e53b3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- correct_date_format: date (nullable = true)\n",
            " |-- incorrect_date_format: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, hour, minute, second\n",
        "cleanDateDf.select(\n",
        "    to_timestamp(col(\"correct_format_date\"), dateFormat).alias(\"timestamp\"),\n",
        "    year(to_timestamp(col(\"correct_format_date\"), dateFormat)).alias(\"Year\"),\n",
        "    month(to_timestamp(col(\"correct_format_date\"), dateFormat)).alias(\"Month\"),\n",
        "    dayofmonth(to_timestamp(col(\"correct_format_date\"), dateFormat)).alias(\"Day\"),\n",
        "    hour(to_timestamp(col(\"correct_format_date\"), dateFormat)).alias(\"Hour\"),\n",
        "    minute(to_timestamp(col(\"correct_format_date\"), dateFormat)).alias(\"minute\"),\n",
        "    second(to_timestamp(col(\"correct_format_date\"), dateFormat).alias(\"timestamp\"))\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "Wj8n1PYNllRM",
        "outputId": "c59e6b63-62d0-4acb-e470-3e5e39953d6f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `correct_format_date` cannot be resolved. Did you mean one of the following? [`correct_date_format`, `incorrect_date_format`].;\n'Project [to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false) AS timestamp#351, year(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false)) AS Year#352, month(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false)) AS Month#353, dayofmonth(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false)) AS Day#354, hour(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false), Some(Etc/UTC)) AS Hour#355, minute(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false), Some(Etc/UTC)) AS minute#356, unresolvedalias(second(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false), Some(Etc/UTC)), Some(org.apache.spark.sql.Column$$Lambda$1567/0x0000000840cc5840@66da335a))]\n+- Project [to_date(2023-12-11, Some(yyyy-dd-MM), Some(Etc/UTC), false) AS correct_date_format#303, to_date(2023-20-13, Some(yyyy-dd-MM), Some(Etc/UTC), false) AS incorrect_date_format#304]\n   +- Range (0, 5, step=1, splits=Some(2))\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2b91573b7950>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayofmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m cleanDateDf.select(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"correct_format_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateFormat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0myear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"correct_format_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateFormat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"correct_format_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateFormat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `correct_format_date` cannot be resolved. Did you mean one of the following? [`correct_date_format`, `incorrect_date_format`].;\n'Project [to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false) AS timestamp#351, year(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false)) AS Year#352, month(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false)) AS Month#353, dayofmonth(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false)) AS Day#354, hour(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false), Some(Etc/UTC)) AS Hour#355, minute(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false), Some(Etc/UTC)) AS minute#356, unresolvedalias(second(to_timestamp('correct_format_date, Some(yyyy-dd-MM), TimestampType, Some(Etc/UTC), false), Some(Etc/UTC)), Some(org.apache.spark.sql.Column$$Lambda$1567/0x0000000840cc5840@66da335a))]\n+- Project [to_date(2023-12-11, Some(yyyy-dd-MM), Some(Etc/UTC), false) AS correct_date_format#303, to_date(2023-20-13, Some(yyyy-dd-MM), Some(Etc/UTC), false) AS incorrect_date_format#304]\n   +- Range (0, 5, step=1, splits=Some(2))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}