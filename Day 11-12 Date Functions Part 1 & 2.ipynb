{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K9R9pyy1gU2Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive\n",
        "from os import truncate\n",
        "# Mount Google Drive with a longer timeout\n",
        "# drive.mount('/content/drive', force_remount=True, timeout_ms=300000)\n",
        "\n",
        "# df_employee_data = \"/content/drive/MyDrive/Colab Notebooks/dataSet/employee_data.csv\"\n",
        "# employeeSechema = StructType([\n",
        "#     StructField(\"ID\",IntegerType() ,True),\n",
        "#     StructField(\"Name\",StringType() ,True),\n",
        "#     StructField(\"Age\",IntegerType() ,True),\n",
        "#     StructField(\"Salary\",FloatType() ,True),\n",
        "#     StructField(\"Joining_Date\",DateType() ,True),\n",
        "#     StructField(\"Department\",StringType() ,True),\n",
        "#     StructField(\"Performance_Rating\",IntegerType() ,True),\n",
        "#     StructField(\"Email\",StringType() ,True),\n",
        "#     StructField(\"Address\",StringType() ,True),\n",
        "#     StructField(\"Phone\",StringType() ,True)\n",
        "\n",
        "# ])\n",
        "# # Load the DataFrame with the defined schema\n",
        "# #df = spark.read.csv(path=df_employee_data, header=True, schema=employeeSechema)\n",
        "# df = spark.read.load(path=\"/content/drive/MyDrive/Colab Notebooks/dataSet/employee_data.csv\", format=\"csv\", header = True, schema=employeeSechema)\n",
        "# df.printSchema()\n",
        "# df.show(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Date Function in Dataframe – Part 1"
      ],
      "metadata": {
        "id": "YVFhJ0j8ZxER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation with Notes\n",
        "1. Creating a Spark Session:\n",
        "  * We begin by creating a Spark session to run the PySpark operations.\n",
        "2. Generating a DataFrame:\n",
        "  * Using spark.range(10) creates a DataFrame with 10 rows and a single column (id) with numbers ranging from 0 to 9.\n",
        "  * Two additional columns are added:\n",
        "    * today: Contains the current date using current_date().\n",
        "    * now: Contains the current timestamp using current_timestamp().\n",
        "3. Date Manipulation Functions:\n",
        "  * date_add: Adds a specified number of days to the date.\n",
        "  * date_sub: Subtracts a specified number of days from the date.\n",
        "  * datediff: Returns the difference in days between two dates.\n",
        "  * months_between: Returns the number of months between two dates.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SMGwfMlKZ4_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateDf = spark.range(10).withColumn(\"Today\", current_date()).withColumn(\"Now\", current_timestamp())\n",
        "dateDf.show(truncate=False)"
      ],
      "metadata": {
        "id": "ebQZ3XkwZzfc",
        "outputId": "9d1d4098-bb0a-4d93-ee30-2b446a1cfe7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+--------------------------+\n",
            "|id |Today     |Now                       |\n",
            "+---+----------+--------------------------+\n",
            "|0  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|1  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|2  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|3  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|4  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|5  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|6  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|7  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|8  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "|9  |2025-03-27|2025-03-27 16:25:33.293214|\n",
            "+---+----------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. date_add and date_sub:"
      ],
      "metadata": {
        "id": "dBi16uQqbf06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateDf.select(\n",
        "    date_sub(col(\"Today\"), 5).alias(\"date_sub_5_days\"),\n",
        "    date_add(col(\"Today\"), 5).alias(\"date_add_5_days\")\n",
        "  ).show()"
      ],
      "metadata": {
        "id": "hDYvMBoJb3_a",
        "outputId": "d273626f-7f9b-492c-b5fe-ca061d859d15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+\n",
            "|date_sub_5_days|date_add_5_days|\n",
            "+---------------+---------------+\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "|     2025-03-22|     2025-04-01|\n",
            "+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Datediff:\n",
        "  * datediff(col(\"week_ago\"), col(\"today\")): Calculates the difference in days between the current date and 7 days ago (i.e., -7).\n"
      ],
      "metadata": {
        "id": "ONCNIRB4c3Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the days diffrence between \"today\" and \"week_ago\" (7 days ago)\n",
        "dateDf.withColumn(\"week_ago\", date_sub(col(\"today\"),7)).select(datediff(col(\"week_ago\"), col(\"today\")).alias(\"days_difference\")).show()"
      ],
      "metadata": {
        "id": "hCwyVCqJckic",
        "outputId": "0007a4d4-30d5-4c76-c2e9-6c5ffc4de235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|days_difference|\n",
            "+---------------+\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "|             -7|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. months_between:\n",
        "* months_between(to_date(lit(\"2016-01-01\")), to_date(lit(\"2017-01-01\")): Calculates the number of months between January 1, 2016, and January 1, 2017, which is -12 months because start_date is earlier than end_date."
      ],
      "metadata": {
        "id": "w70odkFah47k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of months between two specific dates\n",
        "dateDf.select(\n",
        "    to_date(lit(\"2016-01-01\")).alias(\"start_date\"),\n",
        "    to_date(lit(\"2017-01-01\")).alias(\"end_date\")\n",
        ").select(months_between(col(\"start_date\"),col(\"end_date\")).alias(\"months_between\")).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "RUxDyv1dhvT_",
        "outputId": "809ae167-75f3-490d-fe31-c2ae29b6bd6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|months_between|\n",
            "+--------------+\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "|         -12.0|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Date Function in Dataframe – Part 2"
      ],
      "metadata": {
        "id": "zMsrWEKOjRPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Default Date Parsing (to_date):\n",
        "  * When using to_date(), the default date format is yyyy-MM-dd.\n",
        "  * If the format of the string does not match this, PySpark returns null for invalid date parsing."
      ],
      "metadata": {
        "id": "jVqu8w8CjYei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateDf.select(\n",
        "    to_date(lit(\"2016-20-12\")).alias(\"incorrect_date\"),\n",
        "    to_date(lit(\"2025-12-11\")).alias(\"correct_date\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "2YUvqviqjH6p",
        "outputId": "ec3105a6-d9ab-4718-9359-c747178901a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------+\n",
            "|incorrect_date|correct_date|\n",
            "+--------------+------------+\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "|          NULL|  2025-12-11|\n",
            "+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Custom Date Formats:\n",
        "  * You can specify a custom date format using the to_date function by providing a format string, such as yyyy-dd-MM.\n",
        "  * This allows PySpark to correctly parse the dates that deviate from the default format."
      ],
      "metadata": {
        "id": "8wcKrHGOkTw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateFormat = \"yyyy-dd-MM\"\n",
        "cleanDateDf = spark.range(5).select(\n",
        "    to_date(lit()),\n",
        "    to_date(),\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "IRFcDxejkIaT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}